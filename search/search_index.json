{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to shh","text":"<p>Voice transcription CLI powered by OpenAI Whisper</p> <p>Record, transcribe, format. All from your terminal.</p>"},{"location":"#what-is-shh","title":"What is shh?","text":"<p>shh is a command-line tool for recording audio, transcribing it using OpenAI's Whisper API, and optionally formatting or translating the output with AI.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>\ud83c\udfa4 One-command recording - Start recording with <code>shh</code>, stop with Enter \u2728 AI-powered formatting - Clean up transcriptions in casual, business, or neutral styles \ud83c\udf0d Translation - Transcribe and translate to any language \ud83d\udccb Clipboard integration - Results automatically copied for instant use \u26a1 Async architecture - Non-blocking operations for responsive UX \ud83c\udfa8 Beautiful UI - Rich terminal output with live progress indicators</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Install\nuv pip install -e .\n\n# Configure API key\nshh setup\n\n# Start recording (press Enter to stop)\nshh\n\n# Output appears in terminal and clipboard\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Meeting notes - Record conversations and get formatted transcriptions</li> <li>Voice memos - Capture ideas quickly without typing</li> <li>Content creation - Transcribe interviews, podcasts, or videos</li> <li>Multilingual work - Transcribe and translate in one step</li> <li>Accessibility - Convert speech to text for documentation</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>shh follows a pragmatic layered architecture:</p> <pre><code>CLI Layer (Typer)     \u2192 User interaction and commands\n    \u2193\nCore Layer            \u2192 Business logic and orchestration\n    \u2193\nAdapters Layer        \u2192 External APIs, audio hardware, clipboard\n</code></pre> <p>This separation ensures clean dependencies and makes testing straightforward.</p>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Installation Guide - Set up shh on your system</li> <li>Quick Start - Get transcribing in 2 minutes</li> <li>Commands Reference - Learn all available commands</li> <li>Architecture Overview - Understand how shh works</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or higher</li> <li>OpenAI API key</li> <li>Microphone (for recording)</li> </ul>"},{"location":"api-reference/adapters/","title":"Adapters API Reference","text":"<p>External integrations: audio recording, OpenAI APIs, clipboard.</p>"},{"location":"api-reference/adapters/#audio-recorder","title":"Audio Recorder","text":""},{"location":"api-reference/adapters/#shh.adapters.audio.recorder","title":"<code>shh.adapters.audio.recorder</code>","text":""},{"location":"api-reference/adapters/#shh.adapters.audio.recorder.AudioRecorder","title":"<code>AudioRecorder</code>","text":"<p>Async Context Manager for recording audio from the microphone. Usage:     async with AudioRecorder(sample_rate=16000, max_duration=60) as recorder:         # Recording in progress         await do_something()</p> Source code in <code>shh/adapters/audio/recorder.py</code> <pre><code>class AudioRecorder:\n    \"\"\"\n    Async Context Manager for recording audio from the microphone.\n    Usage:\n        async with AudioRecorder(sample_rate=16000, max_duration=60) as recorder:\n            # Recording in progress\n            await do_something()\n    \"\"\"\n\n    MAX_RECORDING_DURATION = 300  # seconds\n\n    def __init__(self, sample_rate: int = SAMPLE_RATE, max_duration: float | None = None) -&gt; None:\n        \"\"\"\n        Initialize the AudioRecorder.\n        \"\"\"\n        self._sample_rate = sample_rate\n        self._max_duration = max_duration or self.MAX_RECORDING_DURATION\n        self._chunks: list[NDArray[np.float32]] = []\n        self._stream: sd.InputStream | None = None\n        self._start_time: float | None = None\n\n    async def __aenter__(self) -&gt; \"AudioRecorder\":\n        \"\"\"\n        Start the audio recording stream.\n        Returns:\n            Self for use within the async context manager.\n        \"\"\"\n\n        def callback(\n            indata: NDArray[np.float32], frames: int, time_info: object, status: sd.CallbackFlags\n        ) -&gt; None:\n            \"\"\"\n            Called by sounddevice for each audio block every 100ms.\n            Args:\n                indata: The recorded audio data.\n                frames: Number of frames.\n                time_info: Time information.\n                status: Status flags.\n            \"\"\"\n            if status:\n                logger.warning(f\"Audio recording status: {status}\")\n\n            self._chunks.append(indata.copy())\n\n        try:\n            self._stream = sd.InputStream(\n                samplerate=self._sample_rate,\n                channels=1,\n                dtype=np.float32,\n                callback=callback,\n            )\n            await asyncio.to_thread(self._stream.start)\n            self._start_time = time.time()\n\n            return self\n        except Exception as e:\n            raise AudioRecordingError(f\"Failed to start audio recording: {e}\") from e\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: object,\n    ) -&gt; None:\n        \"\"\"\n        Stop the audio recording stream and finalize the recording.\n        \"\"\"\n        if self._stream:\n            try:\n                await asyncio.to_thread(self._stream.stop)\n                await asyncio.to_thread(self._stream.close)\n            except Exception as e:\n                logger.error(f\"Error stopping audio stream: {e}\")\n\n    def get_audio(self) -&gt; NDArray[np.float32]:\n        \"\"\"\n        Retrieve the recorded audio data as a single NumPy array.\n        Returns:\n            A NumPy array containing the recorded audio data.\n        \"\"\"\n        if not self._chunks:\n            return np.array([], dtype=np.float32)\n\n        audio = np.concatenate(self._chunks, axis=0)\n        return audio.flatten()\n\n    def elapsed_time(self) -&gt; float:\n        \"\"\"\n        Get the elapsed recording time in seconds.\n        Returns:\n            Elapsed time in seconds.\n        \"\"\"\n        if self._start_time is None:\n            return 0.0\n        return time.time() - self._start_time\n\n    def is_max_duration_reached(self) -&gt; bool:\n        \"\"\"\n        return True if the maximum recording duration has been exceeded.\n        \"\"\"\n        return self.elapsed_time() &gt;= self._max_duration\n</code></pre>"},{"location":"api-reference/adapters/#shh.adapters.audio.recorder.AudioRecorder.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Start the audio recording stream. Returns:     Self for use within the async context manager.</p> Source code in <code>shh/adapters/audio/recorder.py</code> <pre><code>async def __aenter__(self) -&gt; \"AudioRecorder\":\n    \"\"\"\n    Start the audio recording stream.\n    Returns:\n        Self for use within the async context manager.\n    \"\"\"\n\n    def callback(\n        indata: NDArray[np.float32], frames: int, time_info: object, status: sd.CallbackFlags\n    ) -&gt; None:\n        \"\"\"\n        Called by sounddevice for each audio block every 100ms.\n        Args:\n            indata: The recorded audio data.\n            frames: Number of frames.\n            time_info: Time information.\n            status: Status flags.\n        \"\"\"\n        if status:\n            logger.warning(f\"Audio recording status: {status}\")\n\n        self._chunks.append(indata.copy())\n\n    try:\n        self._stream = sd.InputStream(\n            samplerate=self._sample_rate,\n            channels=1,\n            dtype=np.float32,\n            callback=callback,\n        )\n        await asyncio.to_thread(self._stream.start)\n        self._start_time = time.time()\n\n        return self\n    except Exception as e:\n        raise AudioRecordingError(f\"Failed to start audio recording: {e}\") from e\n</code></pre>"},{"location":"api-reference/adapters/#shh.adapters.audio.recorder.AudioRecorder.__aexit__","title":"<code>__aexit__(exc_type, exc_value, traceback)</code>  <code>async</code>","text":"<p>Stop the audio recording stream and finalize the recording.</p> Source code in <code>shh/adapters/audio/recorder.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_value: BaseException | None,\n    traceback: object,\n) -&gt; None:\n    \"\"\"\n    Stop the audio recording stream and finalize the recording.\n    \"\"\"\n    if self._stream:\n        try:\n            await asyncio.to_thread(self._stream.stop)\n            await asyncio.to_thread(self._stream.close)\n        except Exception as e:\n            logger.error(f\"Error stopping audio stream: {e}\")\n</code></pre>"},{"location":"api-reference/adapters/#shh.adapters.audio.recorder.AudioRecorder.__init__","title":"<code>__init__(sample_rate=SAMPLE_RATE, max_duration=None)</code>","text":"<p>Initialize the AudioRecorder.</p> Source code in <code>shh/adapters/audio/recorder.py</code> <pre><code>def __init__(self, sample_rate: int = SAMPLE_RATE, max_duration: float | None = None) -&gt; None:\n    \"\"\"\n    Initialize the AudioRecorder.\n    \"\"\"\n    self._sample_rate = sample_rate\n    self._max_duration = max_duration or self.MAX_RECORDING_DURATION\n    self._chunks: list[NDArray[np.float32]] = []\n    self._stream: sd.InputStream | None = None\n    self._start_time: float | None = None\n</code></pre>"},{"location":"api-reference/adapters/#shh.adapters.audio.recorder.AudioRecorder.elapsed_time","title":"<code>elapsed_time()</code>","text":"<p>Get the elapsed recording time in seconds. Returns:     Elapsed time in seconds.</p> Source code in <code>shh/adapters/audio/recorder.py</code> <pre><code>def elapsed_time(self) -&gt; float:\n    \"\"\"\n    Get the elapsed recording time in seconds.\n    Returns:\n        Elapsed time in seconds.\n    \"\"\"\n    if self._start_time is None:\n        return 0.0\n    return time.time() - self._start_time\n</code></pre>"},{"location":"api-reference/adapters/#shh.adapters.audio.recorder.AudioRecorder.get_audio","title":"<code>get_audio()</code>","text":"<p>Retrieve the recorded audio data as a single NumPy array. Returns:     A NumPy array containing the recorded audio data.</p> Source code in <code>shh/adapters/audio/recorder.py</code> <pre><code>def get_audio(self) -&gt; NDArray[np.float32]:\n    \"\"\"\n    Retrieve the recorded audio data as a single NumPy array.\n    Returns:\n        A NumPy array containing the recorded audio data.\n    \"\"\"\n    if not self._chunks:\n        return np.array([], dtype=np.float32)\n\n    audio = np.concatenate(self._chunks, axis=0)\n    return audio.flatten()\n</code></pre>"},{"location":"api-reference/adapters/#shh.adapters.audio.recorder.AudioRecorder.is_max_duration_reached","title":"<code>is_max_duration_reached()</code>","text":"<p>return True if the maximum recording duration has been exceeded.</p> Source code in <code>shh/adapters/audio/recorder.py</code> <pre><code>def is_max_duration_reached(self) -&gt; bool:\n    \"\"\"\n    return True if the maximum recording duration has been exceeded.\n    \"\"\"\n    return self.elapsed_time() &gt;= self._max_duration\n</code></pre>"},{"location":"api-reference/adapters/#audio-processor","title":"Audio Processor","text":""},{"location":"api-reference/adapters/#shh.adapters.audio.processor","title":"<code>shh.adapters.audio.processor</code>","text":""},{"location":"api-reference/adapters/#shh.adapters.audio.processor.save_audio_to_wav","title":"<code>save_audio_to_wav(audio_data, sample_rate=SAMPLE_RATE)</code>","text":"<p>Save audio data to a temporary WAV file. This function is not responsible for cleaning up the temporary file.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>NDArray[float32]</code> <p>The audio data to save.</p> required <code>sample_rate</code> <code>int</code> <p>The sample rate of the audio data.</p> <code>SAMPLE_RATE</code> <p>Returns:     Path: The path to the saved WAV file. Raises:     AudioProcessingError: If there is an error saving the audio file.</p> Source code in <code>shh/adapters/audio/processor.py</code> <pre><code>def save_audio_to_wav(\n    audio_data: NDArray[np.float32],\n    sample_rate: int = SAMPLE_RATE,\n) -&gt; Path:\n    \"\"\"\n    Save audio data to a temporary WAV file.\n    This function is not responsible for cleaning up the temporary file.\n\n    Args:\n        audio_data (NDArray[np.float32]): The audio data to save.\n        sample_rate (int): The sample rate of the audio data.\n    Returns:\n        Path: The path to the saved WAV file.\n    Raises:\n        AudioProcessingError: If there is an error saving the audio file.\n    \"\"\"\n    try:\n        # Convert float32 audio data to int16\n        audio_int16: NDArray[np.int16] = (audio_data * 32767).astype(np.int16)\n\n        # Create a temporary file\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_file:\n            temp_file_path = Path(temp_file.name)\n\n        # Write the audio data to the WAV file\n        wavfile.write(temp_file_path, sample_rate, audio_int16)\n\n        return temp_file_path\n\n    except Exception as e:\n        raise AudioProcessingError(f\"Failed to save audio to WAV file: {e}\") from e\n</code></pre>"},{"location":"api-reference/adapters/#whisper-client","title":"Whisper Client","text":""},{"location":"api-reference/adapters/#shh.adapters.whisper.client","title":"<code>shh.adapters.whisper.client</code>","text":""},{"location":"api-reference/adapters/#shh.adapters.whisper.client.transcribe_audio","title":"<code>transcribe_audio(audio_file_path, api_key, model='whisper-1')</code>  <code>async</code>","text":"<p>Transcribe audio using OpenAI's Whisper API.</p> Source code in <code>shh/adapters/whisper/client.py</code> <pre><code>async def transcribe_audio(\n    audio_file_path: Path,\n    api_key: str,\n    model: str = \"whisper-1\",\n) -&gt; str:\n    \"\"\"\n    Transcribe audio using OpenAI's Whisper API.\n    \"\"\"\n    client = AsyncOpenAI(api_key=api_key)\n    try:\n        with audio_file_path.open(\"rb\") as audio_file:\n            audio_transcription = await client.audio.transcriptions.create(\n                file=audio_file,\n                model=model,\n            )\n            return audio_transcription.text\n\n    except Exception as e:\n        logger.error(f\"Transcription failed: {e}\")\n        raise TranscriptionError(\"Failed to transcribe audio.\") from e\n</code></pre>"},{"location":"api-reference/adapters/#llm-formatter","title":"LLM Formatter","text":""},{"location":"api-reference/adapters/#shh.adapters.llm.formatter","title":"<code>shh.adapters.llm.formatter</code>","text":""},{"location":"api-reference/adapters/#shh.adapters.llm.formatter.FormattedTranscription","title":"<code>FormattedTranscription</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structured output from LLM formatting.</p> Source code in <code>shh/adapters/llm/formatter.py</code> <pre><code>class FormattedTranscription(BaseModel):\n    \"\"\"Structured output from LLM formatting.\"\"\"\n\n    text: str = Field(..., description=\"The formatted transcription text.\")\n</code></pre>"},{"location":"api-reference/adapters/#shh.adapters.llm.formatter.format_transcription","title":"<code>format_transcription(text, style=TranscriptionStyle.NEUTRAL, api_key='', target_language=None)</code>  <code>async</code>","text":"<p>Format the transcription text using an AI agent based on the specified style.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw transcription text from Whisper</p> required <code>style</code> <code>TranscriptionStyle</code> <p>Formatting style to apply (neutral, casual, business)</p> <code>NEUTRAL</code> <code>api_key</code> <code>str</code> <p>OpenAI API key for LLM calls</p> <code>''</code> <code>target_language</code> <code>str | None</code> <p>Optional language to translate to (e.g., \"English\", \"French\", \"Spanish\")</p> <code>None</code> <p>Returns:</p> Type Description <code>FormattedTranscription</code> <p>FormattedTranscription with styled and optionally translated text</p> Source code in <code>shh/adapters/llm/formatter.py</code> <pre><code>async def format_transcription(\n    text: str,\n    style: TranscriptionStyle = TranscriptionStyle.NEUTRAL,\n    api_key: str = \"\",\n    target_language: str | None = None,\n) -&gt; FormattedTranscription:\n    \"\"\"\n    Format the transcription text using an AI agent based on the specified style.\n\n    Args:\n        text: Raw transcription text from Whisper\n        style: Formatting style to apply (neutral, casual, business)\n        api_key: OpenAI API key for LLM calls\n        target_language: Optional language to translate to (e.g., \"English\", \"French\", \"Spanish\")\n\n    Returns:\n        FormattedTranscription with styled and optionally translated text\n    \"\"\"\n\n    # Handle neutral style - no LLM call needed\n    if style == TranscriptionStyle.NEUTRAL and not target_language:\n        return FormattedTranscription(text=text)\n\n    # For neutral style with translation, use casual prompt\n    system_prompt = STYLE_PROMPTS.get(style, STYLE_PROMPTS[TranscriptionStyle.CASUAL])\n\n    # Build the user prompt\n    user_prompt = f\"Format this transcription: {text}\"\n    if target_language:\n        user_prompt = f\"Format this transcription and translate it to {target_language}: {text}\"\n\n    # Create OpenAI model with API key\n    model = OpenAIChatModel(\"gpt-4o-mini\", provider=OpenAIProvider(api_key=api_key))\n\n    # Create PydanticAI agent\n    agent: Agent[None, FormattedTranscription] = Agent(\n        model,\n        output_type=FormattedTranscription,\n        system_prompt=system_prompt,\n    )\n\n    try:\n        result = await agent.run(user_prompt)\n        return result.output\n    except Exception as e:\n        logger.error(f\"Formatting failed: {e}\")\n        raise FormattingError(f\"Failed to format transcription: {e}\") from e\n</code></pre>"},{"location":"api-reference/adapters/#clipboard-manager","title":"Clipboard Manager","text":""},{"location":"api-reference/adapters/#shh.adapters.clipboard.manager","title":"<code>shh.adapters.clipboard.manager</code>","text":"<p>Clipboard operations for copying transcription results.</p>"},{"location":"api-reference/adapters/#shh.adapters.clipboard.manager.copy_to_clipboard","title":"<code>copy_to_clipboard(text)</code>  <code>async</code>","text":"<p>Copy text to system clipboard.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to copy to clipboard</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>shh/adapters/clipboard/manager.py</code> <pre><code>async def copy_to_clipboard(text: str) -&gt; None:\n    \"\"\"\n    Copy text to system clipboard.\n\n    Args:\n        text: Text to copy to clipboard\n\n    Returns:\n        None\n    \"\"\"\n    # Fail silently if clipboard unavailable\n    with contextlib.suppress(Exception):\n        pyperclip.copy(text)\n</code></pre>"},{"location":"api-reference/cli/","title":"CLI API Reference","text":"<p>User-facing commands. Built with Typer and Rich.</p>"},{"location":"api-reference/cli/#main-app","title":"Main App","text":""},{"location":"api-reference/cli/#shh.cli.app","title":"<code>shh.cli.app</code>","text":"<p>Main CLI application entry point.</p>"},{"location":"api-reference/cli/#shh.cli.app.default_command","title":"<code>default_command(ctx, style=None, translate=None)</code>","text":"<p>Record audio from microphone and transcribe it.</p> <p>\b Press Enter to stop recording (or auto-stop after 5 minutes). Results are automatically copied to clipboard.</p> <p>\b Examples:   shh                           Quick recording with defaults   shh -s casual                 Casual formatting (removes filler words)   shh -s business               Professional formatting   shh -t English                Transcribe and translate to English   shh -s business -t French     Business format + translate to French</p> Source code in <code>shh/cli/app.py</code> <pre><code>@app.callback(invoke_without_command=True)\ndef default_command(\n    ctx: typer.Context,\n    style: Annotated[\n        TranscriptionStyle | None,\n        typer.Option(\n            \"--style\",\n            \"-s\",\n            help=\"Formatting style: neutral (raw), casual (conversational), or business (professional)\",\n        ),\n    ] = None,\n    translate: Annotated[\n        str | None,\n        typer.Option(\n            \"--translate\",\n            \"-t\",\n            help=\"Translate to target language (e.g., 'English', 'French', 'Spanish')\",\n        ),\n    ] = None,\n) -&gt; None:\n    \"\"\"\n    Record audio from microphone and transcribe it.\n\n    \\b\n    Press Enter to stop recording (or auto-stop after 5 minutes).\n    Results are automatically copied to clipboard.\n\n    \\b\n    Examples:\n      shh                           Quick recording with defaults\n      shh -s casual                 Casual formatting (removes filler words)\n      shh -s business               Professional formatting\n      shh -t English                Transcribe and translate to English\n      shh -s business -t French     Business format + translate to French\n    \"\"\"\n    # If a subcommand was invoked, don't run the default\n    if ctx.invoked_subcommand is not None:\n        return\n\n    # Run the async record command\n    asyncio.run(record_command(style=style, translate=translate))\n</code></pre>"},{"location":"api-reference/cli/#shh.cli.app.main","title":"<code>main()</code>","text":"<p>Main entry point for the CLI application.</p> Source code in <code>shh/cli/app.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the CLI application.\"\"\"\n    app()\n</code></pre>"},{"location":"api-reference/cli/#shh.cli.app.setup","title":"<code>setup()</code>","text":"<p>Configure OpenAI API key and settings.</p> Source code in <code>shh/cli/app.py</code> <pre><code>@app.command(name=\"setup\")\ndef setup() -&gt; None:\n    \"\"\"Configure OpenAI API key and settings.\"\"\"\n    setup_command()\n</code></pre>"},{"location":"api-reference/cli/#setup-command","title":"Setup Command","text":""},{"location":"api-reference/cli/#shh.cli.commands.setup","title":"<code>shh.cli.commands.setup</code>","text":"<p>Setup command for configuring the shh CLI.</p>"},{"location":"api-reference/cli/#shh.cli.commands.setup.setup_command","title":"<code>setup_command()</code>","text":"<p>Interactive setup to configure OpenAI API key.</p> <p>Prompts the user for their API key and saves it to the config file.</p> Source code in <code>shh/cli/commands/setup.py</code> <pre><code>def setup_command() -&gt; None:\n    \"\"\"\n    Interactive setup to configure OpenAI API key.\n\n    Prompts the user for their API key and saves it to the config file.\n    \"\"\"\n    console.print(\"\\n[bold]shh Setup[/bold]\", style=\"cyan\")\n    console.print(\"Let's configure your OpenAI API key.\\n\")\n\n    # Prompt for API key (hidden input for security)\n    api_key = typer.prompt(\n        \"Enter your OpenAI API key\",\n        hide_input=True,  # Don't show the key as they type\n    )\n\n    # Validate it's not empty\n    if not api_key or not api_key.strip():\n        console.print(\"[red]Error: API key cannot be empty[/red]\")\n        raise typer.Exit(code=1)\n\n    # Load existing settings or create new ones\n    settings = Settings.load_from_file() or Settings()\n\n    # Update the API key\n    settings.openai_api_key = api_key.strip()\n\n    # Save to file\n    settings.save_to_file()\n    config_path = Settings.get_config_path()\n\n    # Success message with details\n    success_panel = Panel(\n        f\"\"\"[green]Configuration saved successfully![/green]\n\n[bold]Config file:[/bold] {config_path}\n\n[bold]Settings:[/bold]\n  \u2022 OpenAI API Key: sk-***{api_key[-4:]}\n  \u2022 Default style: {settings.default_style}\n  \u2022 Show progress: {settings.show_progress}\n  \u2022 Whisper model: {settings.whisper_model}\n\n[dim]You can now run 'shh' to start recording![/dim]\"\"\",\n        title=\"Setup Complete\",\n        border_style=\"green\",\n    )\n\n    console.print(success_panel)\n</code></pre>"},{"location":"api-reference/cli/#config-command","title":"Config Command","text":""},{"location":"api-reference/cli/#shh.cli.commands.config","title":"<code>shh.cli.commands.config</code>","text":"<p>Config management commands for the shh CLI.</p>"},{"location":"api-reference/cli/#shh.cli.commands.config.config_edit","title":"<code>config_edit()</code>","text":"<p>Open configuration file in $EDITOR.</p> Source code in <code>shh/cli/commands/config.py</code> <pre><code>@config_app.command(name=\"edit\")\ndef config_edit() -&gt; None:\n    \"\"\"Open configuration file in $EDITOR.\"\"\"\n    config_path = Settings.get_config_path()\n\n    if not config_path.exists():\n        console.print(\"[yellow]No configuration file found. Run 'shh setup' first.[/yellow]\")\n        raise typer.Exit(code=1)\n\n    # Get editor from environment\n    editor = os.environ.get(\"EDITOR\") or os.environ.get(\"VISUAL\")\n\n    if not editor:\n        console.print(\"[red]Error: No editor configured.[/red]\")\n        console.print(\"[dim]Set the EDITOR environment variable (e.g., export EDITOR=vim)[/dim]\")\n        raise typer.Exit(code=1)\n\n    # Open in editor\n    try:\n        subprocess.run([editor, str(config_path)], check=True)  # noqa: S603\n        console.print(\"[green]\u2713 Configuration file updated[/green]\")\n    except subprocess.CalledProcessError as e:\n        console.print(f\"[red]Error: Failed to open editor '{editor}'[/red]\")\n        raise typer.Exit(code=1) from e\n    except FileNotFoundError as e:\n        console.print(f\"[red]Error: Editor '{editor}' not found[/red]\")\n        raise typer.Exit(code=1) from e\n</code></pre>"},{"location":"api-reference/cli/#shh.cli.commands.config.config_get","title":"<code>config_get(key)</code>","text":"<p>Get a single configuration value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The setting key to retrieve (e.g., 'default_style')</p> required Source code in <code>shh/cli/commands/config.py</code> <pre><code>@config_app.command(name=\"get\")\ndef config_get(key: str) -&gt; None:\n    \"\"\"Get a single configuration value.\n\n    Args:\n        key: The setting key to retrieve (e.g., 'default_style')\n    \"\"\"\n    settings = Settings.load_from_file()\n\n    if not settings:\n        console.print(\"[red]No configuration found. Run 'shh setup' first.[/red]\")\n        raise typer.Exit(code=1)\n\n    # Get the value\n    try:\n        value = getattr(settings, key)\n\n        # Mask API key\n        if key == \"openai_api_key\" and value:\n            value = f\"sk-***{value[-4:]}\"\n\n        console.print(f\"{key}: {value}\")\n    except AttributeError as e:\n        console.print(f\"[red]Error: Unknown setting '{key}'[/red]\")\n        console.print(f\"[dim]Valid keys: {', '.join(VALID_KEYS.keys())}, openai_api_key[/dim]\")\n        raise typer.Exit(code=1) from e\n</code></pre>"},{"location":"api-reference/cli/#shh.cli.commands.config.config_reset","title":"<code>config_reset()</code>","text":"<p>Reset configuration to defaults (keeps API key).</p> Source code in <code>shh/cli/commands/config.py</code> <pre><code>@config_app.command(name=\"reset\")\ndef config_reset() -&gt; None:\n    \"\"\"Reset configuration to defaults (keeps API key).\"\"\"\n    settings = Settings.load_from_file()\n\n    if not settings:\n        console.print(\"[yellow]No configuration found.[/yellow]\")\n        raise typer.Exit(code=1)\n\n    # Confirm with user\n    confirm = typer.confirm(\n        \"Reset all settings to defaults? (API key will be preserved)\",\n        default=False,\n    )\n\n    if not confirm:\n        console.print(\"[yellow]Reset cancelled.[/yellow]\")\n        raise typer.Exit(code=0)\n\n    # Save the API key\n    api_key = settings.openai_api_key\n\n    # Create new defaults\n    settings = Settings()\n    settings.openai_api_key = api_key\n\n    # Save\n    settings.save_to_file()\n\n    console.print(\"[green]\u2713 Configuration reset to defaults[/green]\")\n    console.print(f\"[dim]API key preserved: sk-***{api_key[-4:] if api_key else 'None'}[/dim]\")\n</code></pre>"},{"location":"api-reference/cli/#shh.cli.commands.config.config_set","title":"<code>config_set(key, value)</code>","text":"<p>Update a configuration setting.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The setting key to update</p> required <code>value</code> <code>str</code> <p>The new value</p> required Source code in <code>shh/cli/commands/config.py</code> <pre><code>@config_app.command(name=\"set\")\ndef config_set(key: str, value: str) -&gt; None:\n    \"\"\"Update a configuration setting.\n\n    Args:\n        key: The setting key to update\n        value: The new value\n    \"\"\"\n    settings = Settings.load_from_file() or Settings()\n\n    # Validate key\n    if key == \"openai_api_key\":\n        console.print(\"[yellow]Use 'shh setup' to update API key.[/yellow]\")\n        raise typer.Exit(code=1)\n\n    if key not in VALID_KEYS:\n        console.print(f\"[red]Error: Unknown setting '{key}'[/red]\")\n        console.print(f\"[dim]Valid keys: {', '.join(VALID_KEYS.keys())}[/dim]\")\n        raise typer.Exit(code=1)\n\n    # Validate and convert value\n    typed_value: TranscriptionStyle | WhisperModel | bool | str | None\n\n    if key == \"default_style\":\n        try:\n            typed_value = TranscriptionStyle(value)\n        except ValueError as e:\n            console.print(f\"[red]Error: Invalid style '{value}'[/red]\")\n            valid_styles = [s.value for s in TranscriptionStyle]\n            console.print(f\"[dim]Valid styles: {', '.join(valid_styles)}[/dim]\")\n            raise typer.Exit(code=1) from e\n    elif key == \"default_translation_language\":\n        # Support clearing the value with special strings\n        typed_value = None if value.lower() in (\"none\", \"null\", \"\") else value\n    elif key == \"show_progress\":\n        if value.lower() not in (\"true\", \"false\"):\n            console.print(\"[red]Error: show_progress must be 'true' or 'false'[/red]\")\n            raise typer.Exit(code=1)\n        typed_value = value.lower() == \"true\"\n    elif key == \"whisper_model\":\n        try:\n            typed_value = WhisperModel(value)\n        except ValueError as e:\n            console.print(f\"[red]Error: Invalid model '{value}'[/red]\")\n            valid_models = [m.value for m in WhisperModel]\n            console.print(f\"[dim]Valid models: {', '.join(valid_models)}[/dim]\")\n            raise typer.Exit(code=1) from e\n    else:\n        typed_value = value\n\n    # Update setting\n    setattr(settings, key, typed_value)\n    settings.save_to_file()\n\n    console.print(f\"[green]\u2713 Updated {key} = {typed_value}[/green]\")\n</code></pre>"},{"location":"api-reference/cli/#shh.cli.commands.config.config_show","title":"<code>config_show()</code>","text":"<p>Display current configuration settings.</p> Source code in <code>shh/cli/commands/config.py</code> <pre><code>@config_app.command(name=\"show\")\ndef config_show() -&gt; None:\n    \"\"\"Display current configuration settings.\"\"\"\n    settings = Settings.load_from_file()\n\n    if not settings:\n        console.print(\"[yellow]No configuration found. Run 'shh setup' first.[/yellow]\")\n        raise typer.Exit(code=1)\n\n    # Create a table for settings\n    table = Table(title=\"Configuration Settings\", show_header=True, header_style=\"bold cyan\")\n    table.add_column(\"Setting\", style=\"cyan\", no_wrap=True)\n    table.add_column(\"Value\", style=\"green\")\n\n    # Mask API key\n    api_key_display = (\n        f\"sk-***{settings.openai_api_key[-4:]}\"\n        if settings.openai_api_key\n        else \"[red]Not configured[/red]\"\n    )\n\n    table.add_row(\"openai_api_key\", api_key_display)\n    table.add_row(\"default_style\", str(settings.default_style))\n    table.add_row(\n        \"default_translation_language\",\n        settings.default_translation_language or \"[dim]None[/dim]\",\n    )\n    table.add_row(\"show_progress\", str(settings.show_progress))\n    table.add_row(\"whisper_model\", str(settings.whisper_model))\n    table.add_row(\"default_output\", \", \".join(settings.default_output))\n\n    config_path = Settings.get_config_path()\n    console.print()\n    console.print(table)\n    console.print(f\"\\n[dim]Config file: {config_path}[/dim]\")\n</code></pre>"},{"location":"api-reference/cli/#record-command","title":"Record Command","text":""},{"location":"api-reference/cli/#shh.cli.commands.record","title":"<code>shh.cli.commands.record</code>","text":"<p>Recording command for the shh CLI.</p>"},{"location":"api-reference/cli/#shh.cli.commands.record.record_command","title":"<code>record_command(style=None, translate=None)</code>  <code>async</code>","text":"<p>Record audio, transcribe, and optionally format/translate.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>TranscriptionStyle | None</code> <p>Formatting style to apply (overrides config default)</p> <code>None</code> <code>translate</code> <code>str | None</code> <p>Target language for translation</p> <code>None</code> Source code in <code>shh/cli/commands/record.py</code> <pre><code>async def record_command(\n    style: TranscriptionStyle | None = None,\n    translate: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Record audio, transcribe, and optionally format/translate.\n\n    Args:\n        style: Formatting style to apply (overrides config default)\n        translate: Target language for translation\n    \"\"\"\n    # Load settings\n    settings = Settings.load_from_file()\n    if not settings or not settings.openai_api_key:\n        console.print(\"[red]Error: No API key found.[/red]\")\n        console.print(\"[dim]Run 'shh setup' to configure your OpenAI API key.[/dim]\")\n        sys.exit(1)\n\n    # Use provided options or fall back to config defaults\n    formatting_style = style if style is not None else settings.default_style\n    target_language = translate if translate is not None else settings.default_translation_language\n\n    # Step 1: Recording\n    console.print(\"\\n[bold cyan]Recording...[/bold cyan]\")\n    console.print(\"[dim]Press Enter to stop recording[/dim]\\n\")\n\n    try:\n        async with AudioRecorder() as recorder:\n            # Start waiting for Enter in background\n            enter_task = asyncio.create_task(wait_for_enter())\n\n            # Show live progress\n            with Live(auto_refresh=False, console=console) as live:\n                while not enter_task.done() and not recorder.is_max_duration_reached():\n                    elapsed = recorder.elapsed_time()\n                    max_duration = recorder._max_duration\n\n                    # Create progress text\n                    progress = Text()\n                    progress.append(\"Recording... \", style=\"bold green\")\n                    progress.append(f\"{elapsed:.1f}s \", style=\"cyan\")\n                    progress.append(f\"/ {max_duration:.0f}s\", style=\"dim\")\n\n                    live.update(progress)\n                    live.refresh()\n                    await asyncio.sleep(0.1)\n\n                # Check if max duration reached\n                if recorder.is_max_duration_reached():\n                    console.print(\n                        \"\\n[yellow]Maximum recording duration reached (5 minutes)[/yellow]\"\n                    )\n\n            # Cancel Enter task if we hit max duration\n            if not enter_task.done():\n                enter_task.cancel()\n                with contextlib.suppress(asyncio.CancelledError):\n                    await enter_task\n\n            # Get recorded audio\n            audio_data = recorder.get_audio()\n\n    except KeyboardInterrupt:\n        console.print(\"\\n[yellow]Recording cancelled.[/yellow]\")\n        sys.exit(130)  # Standard exit code for Ctrl+C\n\n    # Check if we got any audio\n    if len(audio_data) == 0:\n        console.print(\"[yellow]No audio recorded.[/yellow]\")\n        sys.exit(1)\n\n    # Step 2: Save to WAV\n    console.print(\"\\n[cyan]Saving audio...[/cyan]\")\n    audio_file_path = save_audio_to_wav(audio_data)\n\n    try:\n        # Step 3: Transcribe\n        console.print(\"[cyan]Transcribing with Whisper...[/cyan]\")\n        transcription = await transcribe_audio(\n            audio_file_path=audio_file_path,\n            api_key=settings.openai_api_key,\n        )\n\n        # Step 4: Format/Translate (if requested)\n        if formatting_style != TranscriptionStyle.NEUTRAL or target_language:\n            if target_language:\n                console.print(f\"[cyan]Formatting and translating to {target_language}...[/cyan]\")\n            else:\n                console.print(f\"[cyan]Formatting ({formatting_style})...[/cyan]\")\n\n            formatted = await format_transcription(\n                transcription,\n                style=formatting_style,\n                api_key=settings.openai_api_key,\n                target_language=target_language,\n            )\n            final_text = formatted.text\n        else:\n            final_text = transcription\n\n        # Step 5: Copy to clipboard\n        clipboard_success = True\n        try:\n            pyperclip.copy(final_text)\n        except Exception as e:\n            clipboard_success = False\n            console.print(f\"[yellow]Warning: Could not copy to clipboard: {e}[/yellow]\")\n\n        # Step 6: Display result\n        console.print()\n        result_panel = Panel(\n            final_text,\n            title=\"Transcription\" + (\" (copied to clipboard)\" if clipboard_success else \"\"),\n            border_style=\"green\" if clipboard_success else \"yellow\",\n        )\n        console.print(result_panel)\n        console.print()\n\n    finally:\n        # Cleanup temp file\n        audio_file_path.unlink(missing_ok=True)\n</code></pre>"},{"location":"api-reference/cli/#shh.cli.commands.record.wait_for_enter","title":"<code>wait_for_enter()</code>  <code>async</code>","text":"<p>Wait for user to press Enter (runs in thread pool).</p> Source code in <code>shh/cli/commands/record.py</code> <pre><code>async def wait_for_enter() -&gt; None:\n    \"\"\"Wait for user to press Enter (runs in thread pool).\"\"\"\n    loop = asyncio.get_running_loop()\n    await loop.run_in_executor(None, sys.stdin.readline)\n</code></pre>"},{"location":"api-reference/core/","title":"Core API Reference","text":"<p>Business logic and domain models.</p>"},{"location":"api-reference/core/#styles","title":"Styles","text":""},{"location":"api-reference/core/#shh.core.styles","title":"<code>shh.core.styles</code>","text":"<p>Transcription style definitions.</p>"},{"location":"api-reference/core/#shh.core.styles.TranscriptionStyle","title":"<code>TranscriptionStyle</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available transcription formatting styles.</p> Source code in <code>shh/core/styles.py</code> <pre><code>class TranscriptionStyle(StrEnum):\n    \"\"\"Available transcription formatting styles.\"\"\"\n\n    NEUTRAL = \"neutral\"\n    CASUAL = \"casual\"\n    BUSINESS = \"business\"\n</code></pre>"},{"location":"architecture/design-decisions/","title":"Design Decisions","text":"<p>This document explains the \"why\" behind key architectural and implementation choices in shh.</p>"},{"location":"architecture/design-decisions/#architecture-decisions","title":"Architecture Decisions","text":""},{"location":"architecture/design-decisions/#why-layered-architecture","title":"Why Layered Architecture?","text":"<p>Decision: Separate CLI, Core, and Adapters layers with unidirectional dependencies.</p> <p>Rationale:</p> <ul> <li>Testability: Core logic can be tested without mocking external APIs</li> <li>Flexibility: Could swap Typer for argparse, or add a web UI</li> <li>Clarity: Clear separation of concerns makes code easier to understand</li> <li>Maintenance: Changes to UI don't affect business logic</li> </ul> <p>Trade-offs:</p> <ul> <li>\u2705 Easier to test and maintain</li> <li>\u2705 Framework-independent core</li> <li>\u26a0\ufe0f More boilerplate (but not much in this small app)</li> </ul> <p>Alternatives considered:</p> <ul> <li>Single-file script: Too messy for a production app</li> <li>Full hexagonal architecture: Overkill for a CLI tool</li> </ul>"},{"location":"architecture/design-decisions/#why-pragmatic-vs-pure-clean-architecture","title":"Why Pragmatic vs. Pure Clean Architecture?","text":"<p>Decision: Orchestration happens in CLI layer, not Core layer.</p> <p>Rationale:</p> <ul> <li>shh is small (&lt; 1000 lines of code)</li> <li>Orchestration logic is tightly coupled to CLI UX</li> <li>Moving orchestration to Core would add complexity without benefit</li> </ul> <p>When to move to Core:</p> <ul> <li>If we add a web UI (need shared orchestration)</li> <li>If orchestration logic becomes complex</li> <li>If we need to test orchestration without CLI</li> </ul> <p>Current approach:</p> <pre><code># CLI layer handles orchestration (pragmatic)\nasync def record_command(...):\n    audio = await record_audio()\n    text = await transcribe_audio(audio)\n    formatted = await format_transcription(text)\n    await copy_to_clipboard(formatted)\n</code></pre> <p>Pure approach (not used):</p> <pre><code># Core layer handles orchestration (overkill for now)\nclass TranscriptionService:\n    def __init__(self, whisper, llm, clipboard):\n        ...\n\n    async def transcribe(self, audio):\n        text = await self.whisper.transcribe(audio)\n        formatted = await self.llm.format(text)\n        await self.clipboard.copy(formatted)\n        return formatted\n</code></pre>"},{"location":"architecture/design-decisions/#technology-choices","title":"Technology Choices","text":""},{"location":"architecture/design-decisions/#why-typer-over-argparse","title":"Why Typer over argparse?","text":"<p>Decision: Use Typer for CLI framework.</p> <p>Rationale:</p> <ul> <li>Type hints: Typer uses Python type hints for argument parsing</li> <li>Automatic help: Generates beautiful help messages</li> <li>Subcommands: Clean syntax for command groups (<code>config show</code>, <code>config set</code>)</li> <li>Rich integration: Works seamlessly with Rich for terminal UI</li> </ul> <p>Trade-offs:</p> <ul> <li>\u2705 Less boilerplate than argparse</li> <li>\u2705 Better UX (help messages, validation)</li> <li>\u26a0\ufe0f Extra dependency (but lightweight)</li> </ul> <p>Alternatives considered:</p> <ul> <li>argparse: Built-in but verbose and less ergonomic</li> <li>click: Popular but less type-safe than Typer</li> </ul>"},{"location":"architecture/design-decisions/#why-rich-for-terminal-ui","title":"Why Rich for Terminal UI?","text":"<p>Decision: Use Rich for all terminal output.</p> <p>Rationale:</p> <ul> <li>Beautiful output: Colors, tables, panels, progress bars</li> <li>Live updates: Real-time progress display while recording</li> <li>Minimal effort: Simple API for complex formatting</li> <li>Professional look: Makes the CLI feel polished</li> </ul> <p>Examples:</p> <pre><code># Rich table\ntable = Table(title=\"Configuration\")\ntable.add_column(\"Setting\", style=\"cyan\")\nconsole.print(table)\n\n# Rich panel\npanel = Panel(\"Success!\", title=\"Setup Complete\", border_style=\"green\")\nconsole.print(panel)\n\n# Rich live display\nwith Live(auto_refresh=False) as live:\n    live.update(Text(\"Recording... 12.3s\"))\n</code></pre> <p>Alternatives considered:</p> <ul> <li>Plain print(): Works but looks amateur</li> <li>colorama: Colors only, no tables or live updates</li> <li>blessed: More complex API</li> </ul>"},{"location":"architecture/design-decisions/#why-pydanticai-over-langchain","title":"Why PydanticAI over LangChain?","text":"<p>Decision: Use PydanticAI for LLM formatting.</p> <p>Rationale:</p> <ul> <li>Structured outputs: Pydantic models ensure valid responses</li> <li>Type safety: Full type hints, integrates with mypy</li> <li>Simplicity: Less complex than LangChain for this use case</li> <li>Modern: Built by Pydantic team, first-class async support</li> </ul> <p>Example:</p> <pre><code>class FormattedTranscription(BaseModel):\n    text: str\n\nagent = Agent(model, result_type=FormattedTranscription)\nresult = await agent.run(f\"Format this: {text}\")\n# result.output.text is guaranteed to be a string\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Simpler than LangChain for our needs</li> <li>\u2705 Type-safe outputs</li> <li>\u26a0\ufe0f Newer library (less mature)</li> </ul> <p>Alternatives considered:</p> <ul> <li>LangChain: Too heavyweight, complex API</li> <li>Direct OpenAI SDK: No structured outputs</li> </ul>"},{"location":"architecture/design-decisions/#why-sounddevice-over-pyaudio","title":"Why sounddevice over pyaudio?","text":"<p>Decision: Use sounddevice for audio recording.</p> <p>Rationale:</p> <ul> <li>Cross-platform: Works on macOS, Linux, Windows</li> <li>NumPy integration: Returns audio as NumPy arrays</li> <li>Modern: Active development, Python 3+ focused</li> <li>Simple API: Easy to use for basic recording</li> </ul> <p>Example:</p> <pre><code>import sounddevice as sd\n\n# Record audio\naudio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\nsd.wait()  # Wait until recording is finished\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Simpler than pyaudio</li> <li>\u2705 Better documentation</li> <li>\u26a0\ufe0f Requires PortAudio system library</li> </ul> <p>Alternatives considered:</p> <ul> <li>pyaudio: More complex, less Pythonic API</li> <li>python-sounddevice: Same as sounddevice (it's an alias)</li> </ul>"},{"location":"architecture/design-decisions/#implementation-patterns","title":"Implementation Patterns","text":""},{"location":"architecture/design-decisions/#why-asyncawait","title":"Why Async/Await?","text":"<p>Decision: Use async/await for all I/O operations.</p> <p>Rationale:</p> <ul> <li>Non-blocking: API calls don't freeze the UI</li> <li>Responsive UX: Live progress updates while recording</li> <li>Modern Python: async/await is the standard for I/O-bound tasks</li> </ul> <p>Where async is used:</p> <ul> <li>Recording audio (async context manager)</li> <li>API calls (Whisper, GPT)</li> <li>Clipboard operations</li> <li>File I/O (when possible)</li> </ul> <p>Bridge to sync:</p> <pre><code># Typer callbacks are sync, bridge to async backend\ndef default_command(...):\n    asyncio.run(record_command(...))\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Better UX (non-blocking operations)</li> <li>\u2705 Modern Python patterns</li> <li>\u26a0\ufe0f Slightly more complex (async/await syntax)</li> </ul>"},{"location":"architecture/design-decisions/#why-press-enter-not-ctrlc-to-stop","title":"Why Press Enter (not Ctrl+C) to Stop?","text":"<p>Decision: Use Enter key to stop recording, not Ctrl+C.</p> <p>Rationale:</p> <ul> <li>Intuitive: Enter is a natural \"done\" signal</li> <li>No signal handling: Ctrl+C sends SIGINT, complicates error handling</li> <li>Graceful shutdown: Enter allows clean async cancellation</li> <li>User testing: Felt more natural than Ctrl+C</li> </ul> <p>Implementation:</p> <pre><code>async def wait_for_enter():\n    loop = asyncio.get_running_loop()\n    # Run blocking stdin.readline in thread pool\n    await loop.run_in_executor(None, sys.stdin.readline)\n\nenter_task = asyncio.create_task(wait_for_enter())\nwhile not enter_task.done():\n    # Continue recording\n    await asyncio.sleep(0.1)\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Cleaner async code</li> <li>\u2705 More intuitive UX</li> <li>\u26a0\ufe0f Different from typical CLI tools (which use Ctrl+C)</li> </ul> <p>Alternative considered:</p> <ul> <li>Ctrl+C: More common but requires signal handling and is less graceful</li> </ul>"},{"location":"architecture/design-decisions/#why-temporary-files-for-audio","title":"Why Temporary Files for Audio?","text":"<p>Decision: Save audio to temporary WAV files, delete immediately after transcription.</p> <p>Rationale:</p> <ul> <li>Whisper API requirement: Requires file upload (not raw bytes)</li> <li>Disk space: Auto-cleanup prevents accumulation</li> <li>Security: Temporary files are automatically cleaned up</li> </ul> <p>Implementation:</p> <pre><code>try:\n    wav_path = save_audio_to_wav(audio_data)\n    text = await transcribe_audio(wav_path, api_key)\nfinally:\n    wav_path.unlink(missing_ok=True)  # Always delete\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Required by API</li> <li>\u2705 Auto-cleanup prevents disk bloat</li> <li>\u26a0\ufe0f Temporary I/O overhead (minimal)</li> </ul> <p>Alternative considered:</p> <ul> <li>Persistent files: Would require manual cleanup or config for storage location</li> </ul>"},{"location":"architecture/design-decisions/#why-enum-for-transcriptionstyle","title":"Why Enum for TranscriptionStyle?","text":"<p>Decision: Use Python Enum for style choices.</p> <p>Rationale:</p> <ul> <li>Type safety: Can't pass invalid styles</li> <li>IDE autocomplete: Enum members appear in autocomplete</li> <li>Validation: Automatic validation in Pydantic models</li> <li>Self-documenting: All valid values in one place</li> </ul> <p>Example:</p> <pre><code>class TranscriptionStyle(str, Enum):\n    NEUTRAL = \"neutral\"\n    CASUAL = \"casual\"\n    BUSINESS = \"business\"\n\n# Type-safe usage\nstyle = TranscriptionStyle.CASUAL\n\n# Pydantic validates automatically\nclass Settings(BaseModel):\n    default_style: TranscriptionStyle\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Type-safe</li> <li>\u2705 Validated automatically</li> <li>\u26a0\ufe0f Slightly more verbose than plain strings</li> </ul> <p>Alternative considered:</p> <ul> <li>Plain strings: Easier but error-prone (typos not caught)</li> </ul>"},{"location":"architecture/design-decisions/#why-pydantic-settings-for-configuration","title":"Why pydantic-settings for Configuration?","text":"<p>Decision: Use pydantic-settings for config management.</p> <p>Rationale:</p> <ul> <li>Type safety: Settings are typed and validated</li> <li>Environment variables: Automatic parsing with <code>SHH_</code> prefix</li> <li>Multiple sources: Supports env vars, JSON files, defaults</li> <li>Validation: Automatic validation on load</li> <li>Platform-agnostic: Works across macOS, Linux, Windows</li> </ul> <p>Example:</p> <pre><code>class Settings(BaseSettings):\n    openai_api_key: str = \"\"\n    default_style: TranscriptionStyle = TranscriptionStyle.NEUTRAL\n\n    class Config:\n        env_prefix = \"SHH_\"  # SHH_OPENAI_API_KEY\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Type-safe configuration</li> <li>\u2705 Automatic validation</li> <li>\u26a0\ufe0f Extra dependency (but small)</li> </ul> <p>Alternatives considered:</p> <ul> <li>ConfigParser: Built-in but less type-safe</li> <li>python-dotenv: Only handles .env files</li> <li>YAML: Requires extra library, more complex</li> </ul>"},{"location":"architecture/design-decisions/#testing-decisions","title":"Testing Decisions","text":""},{"location":"architecture/design-decisions/#why-no-e2e-tests","title":"Why No E2E Tests?","text":"<p>Decision: No end-to-end tests with real APIs in CI.</p> <p>Rationale:</p> <ul> <li>Cost: Real API calls cost money (OpenAI charges per request)</li> <li>Speed: API calls are slow, would make CI sluggish</li> <li>Reliability: External APIs can fail, causing flaky tests</li> <li>Coverage: Integration tests with mocks provide 80%+ coverage</li> </ul> <p>What we test instead:</p> <ul> <li>Unit tests: Core logic, isolated functions</li> <li>Integration tests: Full pipeline with mocked APIs</li> </ul> <p>Example:</p> <pre><code># Integration test with mocked Whisper API\nwith patch(\"shh.adapters.whisper.client.AsyncOpenAI\") as mock:\n    mock_instance = mock.return_value\n    mock_instance.audio.transcriptions.create = AsyncMock(\n        return_value=MagicMock(text=\"Hello world\")\n    )\n\n    result = await transcribe_audio(wav_path, \"sk-test-key\")\n    assert result == \"Hello world\"\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Fast CI (no API waits)</li> <li>\u2705 Free (no API costs)</li> <li>\u26a0\ufe0f Doesn't catch API changes (mitigated by integration tests)</li> </ul> <p>When to add E2E:</p> <ul> <li>If we see production bugs not caught by mocks</li> <li>For release validation (not in regular CI)</li> </ul>"},{"location":"architecture/design-decisions/#future-considerations","title":"Future Considerations","text":""},{"location":"architecture/design-decisions/#when-to-add-a-database","title":"When to Add a Database?","text":"<p>Currently: Settings stored in JSON file.</p> <p>Add a database when:</p> <ul> <li>We need to store transcription history</li> <li>We add user accounts or multi-user support</li> <li>We need search or querying of past transcriptions</li> </ul> <p>Likely choice: SQLite (local, no server needed)</p>"},{"location":"architecture/design-decisions/#when-to-add-a-web-ui","title":"When to Add a Web UI?","text":"<p>Currently: CLI only.</p> <p>Add a web UI when:</p> <ul> <li>Users request it (not yet)</li> <li>We need remote access (record on server, access from browser)</li> </ul> <p>Architecture impact:</p> <ul> <li>Move orchestration from CLI to Core layer</li> <li>Add FastAPI or Flask adapter layer</li> <li>Shared Core logic between CLI and Web</li> </ul>"},{"location":"architecture/design-decisions/#when-to-support-local-whisper","title":"When to Support Local Whisper?","text":"<p>Currently: OpenAI API only (requires internet and API key).</p> <p>Add local Whisper when:</p> <ul> <li>Users request offline support</li> <li>Privacy concerns arise (local processing)</li> </ul> <p>Implementation:</p> <ul> <li>Add <code>LocalWhisperAdapter</code> implementing same interface</li> <li>Configuration setting to choose API vs. Local</li> <li>Download model on first run</li> </ul>"},{"location":"architecture/design-decisions/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Overview - High-level architecture</li> <li>Testing Architecture - Testing strategy and patterns</li> <li>API Reference - Detailed code documentation</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Three layers: CLI, Core, Adapters.</p>"},{"location":"architecture/overview/#layered-architecture","title":"Layered Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              CLI Layer (Typer)                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   app.py  \u2502  \u2502  setup  \u2502  \u2502  config  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502        \u2502             \u2502            \u2502            \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502 depends on\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Core Layer (Business Logic)          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   styles.py  \u2502   \u2502  orchestration   \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502 depends on\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Adapters Layer (External I/O)           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 audio \u2502  \u2502 whisper  \u2502  \u2502 llm  \u2502  \u2502clipboard\u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u2193\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502  External Dependencies    \u2502\n        \u2502  \u2022 OpenAI APIs           \u2502\n        \u2502  \u2022 sounddevice           \u2502\n        \u2502  \u2022 pyperclip             \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#dependency-rule","title":"Dependency Rule","text":"<p>Flow is unidirectional: CLI \u2192 Core \u2192 Adapters</p> <ul> <li>CLI can import Core and Adapters</li> <li>Core cannot import CLI or Adapters</li> <li>Adapters cannot import Core or CLI</li> </ul> <p>This ensures:</p> <ul> <li>Clean separation of concerns</li> <li>Easy testing (core logic has no external dependencies)</li> <li>Framework independence (could swap Typer for argparse)</li> </ul>"},{"location":"architecture/overview/#layer-responsibilities","title":"Layer Responsibilities","text":""},{"location":"architecture/overview/#cli-layer","title":"CLI Layer","text":"<p>Purpose: User interaction, command handling, terminal UI</p> <p>Responsibilities:</p> <ul> <li>Parse command-line arguments (Typer)</li> <li>Display output with Rich (colors, tables, panels)</li> <li>Orchestrate workflow by calling Core and Adapters</li> <li>Bridge sync (Typer) to async (backend) with <code>asyncio.run()</code></li> <li>Handle user-facing error messages</li> </ul> <p>Key files:</p> <ul> <li><code>cli/app.py</code> - Main CLI entry point</li> <li><code>cli/commands/setup.py</code> - API key setup wizard</li> <li><code>cli/commands/config.py</code> - Configuration management</li> <li><code>cli/commands/record.py</code> - Recording workflow</li> </ul> <p>Dependencies:</p> <ul> <li>Typer (CLI framework)</li> <li>Rich (terminal formatting)</li> <li>Core and Adapters layers</li> </ul>"},{"location":"architecture/overview/#core-layer","title":"Core Layer","text":"<p>Purpose: Business logic and orchestration rules</p> <p>Responsibilities:</p> <ul> <li>Define domain models (TranscriptionStyle enum)</li> <li>Contain business rules (when to format, when to translate)</li> <li>Pure logic with no I/O operations</li> <li>Framework-agnostic (no Typer/Rich imports)</li> </ul> <p>Key files:</p> <ul> <li><code>core/styles.py</code> - TranscriptionStyle enum</li> </ul> <p>Dependencies:</p> <ul> <li>Standard library only</li> <li>Pydantic (for types, but not I/O)</li> </ul> <p>Note: Currently, orchestration happens in the CLI layer (<code>record.py</code>). This is a pragmatic choice for simplicity. In a larger app, orchestration would move to Core.</p>"},{"location":"architecture/overview/#adapters-layer","title":"Adapters Layer","text":"<p>Purpose: All external I/O and integrations</p> <p>Responsibilities:</p> <ul> <li>Audio recording (sounddevice)</li> <li>File I/O (scipy for WAV files)</li> <li>OpenAI Whisper API calls (httpx)</li> <li>LLM formatting with PydanticAI (OpenAI GPT)</li> <li>Clipboard operations (pyperclip)</li> <li>Translate external errors to domain exceptions</li> </ul> <p>Key files:</p> <ul> <li><code>adapters/audio/recorder.py</code> - Microphone recording</li> <li><code>adapters/audio/processor.py</code> - WAV file operations</li> <li><code>adapters/whisper/client.py</code> - Whisper API client</li> <li><code>adapters/llm/formatter.py</code> - PydanticAI formatting agent</li> <li><code>adapters/clipboard/manager.py</code> - Clipboard wrapper</li> </ul> <p>Dependencies:</p> <ul> <li>sounddevice, scipy (audio)</li> <li>httpx, openai (APIs)</li> <li>pydantic-ai (LLM agent)</li> <li>pyperclip (clipboard)</li> </ul>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#recording-workflow","title":"Recording Workflow","text":"<pre><code>User runs: shh --style casual --translate English\n\n1. CLI Layer (app.py)\n   \u251c\u2500 Parse args (Typer)\n   \u251c\u2500 Validate API key exists\n   \u2514\u2500 Call asyncio.run(record_command(...))\n\n2. CLI Layer (record.py)\n   \u251c\u2500 Create AudioRecorder\n   \u251c\u2500 Display progress (Rich Live)\n   \u2514\u2500 Wait for Enter or max duration\n\n3. Adapters Layer (audio/recorder.py)\n   \u2514\u2500 Record audio chunks (sounddevice)\n\n4. Adapters Layer (audio/processor.py)\n   \u2514\u2500 Save to WAV file (scipy)\n\n5. Adapters Layer (whisper/client.py)\n   \u2514\u2500 Transcribe audio (OpenAI Whisper API)\n\n6. Adapters Layer (llm/formatter.py)\n   \u251c\u2500 Translate to English (PydanticAI + GPT)\n   \u2514\u2500 Format with casual style (PydanticAI + GPT)\n\n7. Adapters Layer (clipboard/manager.py)\n   \u2514\u2500 Copy to clipboard (pyperclip)\n\n8. CLI Layer (record.py)\n   \u251c\u2500 Display result (Rich Panel)\n   \u2514\u2500 Clean up temp WAV file\n</code></pre>"},{"location":"architecture/overview/#configuration-workflow","title":"Configuration Workflow","text":"<pre><code>User runs: shh config set default_style casual\n\n1. CLI Layer (config.py)\n   \u251c\u2500 Parse args (key, value)\n   \u251c\u2500 Validate key exists\n   \u2514\u2500 Validate value is valid enum\n\n2. Config Layer (settings.py)\n   \u251c\u2500 Load existing config\n   \u251c\u2500 Update setting\n   \u2514\u2500 Save to JSON file\n\n3. Config Layer (storage.py)\n   \u2514\u2500 Write JSON to platform-specific path\n</code></pre>"},{"location":"architecture/overview/#configuration-architecture","title":"Configuration Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Configuration Priority              \u2502\n\u2502  1. CLI Flags        (--style casual)       \u2502\n\u2502  2. Environment Vars (SHH_DEFAULT_STYLE)    \u2502\n\u2502  3. Config File      (config.json)          \u2502\n\u2502  4. Defaults         (hardcoded)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      pydantic-settings (Settings class)     \u2502\n\u2502  \u2022 Type validation                          \u2502\n\u2502  \u2022 Environment variable parsing             \u2502\n\u2502  \u2022 Default values                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Platform-specific Storage           \u2502\n\u2502  macOS:   ~/Library/Application Support/   \u2502\n\u2502  Linux:   ~/.config/shh/                   \u2502\n\u2502  Windows: %APPDATA%\\shh\\                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#async-architecture","title":"Async Architecture","text":""},{"location":"architecture/overview/#why-async","title":"Why Async?","text":"<ul> <li>Non-blocking I/O: API calls don't freeze the UI</li> <li>Responsive UX: Live progress updates while recording</li> <li>Efficient: Multiple operations can run concurrently</li> </ul>"},{"location":"architecture/overview/#async-patterns","title":"Async Patterns","text":"<p>1. Async Context Managers</p> <pre><code>async with AudioRecorder() as recorder:\n    # Recording happens in background\n    await asyncio.sleep(duration)\n# Cleanup happens automatically\n</code></pre> <p>2. Thread Pool for Blocking I/O</p> <pre><code># stdin.readline() is blocking, run in thread pool\nloop = asyncio.get_running_loop()\nawait loop.run_in_executor(None, sys.stdin.readline)\n</code></pre> <p>3. Task Management</p> <pre><code># Create background tasks\nenter_task = asyncio.create_task(wait_for_enter())\n\n# Cancel if not needed\nif not enter_task.done():\n    enter_task.cancel()\n    with contextlib.suppress(asyncio.CancelledError):\n        await enter_task\n</code></pre>"},{"location":"architecture/overview/#type-safety","title":"Type Safety","text":"<p>All code is typed with mypy --strict:</p> <ul> <li>Every function has type hints</li> <li>No <code>Any</code> without justification</li> <li>Pydantic models for structured data</li> <li>Enums for finite choices (TranscriptionStyle, WhisperModel)</li> </ul> <p>Benefits:</p> <ul> <li>Catch bugs at development time</li> <li>Self-documenting code</li> <li>IDE autocomplete and hints</li> <li>Refactoring confidence</li> </ul>"},{"location":"architecture/overview/#error-handling","title":"Error Handling","text":"<p>Philosophy: Fail fast with clear messages</p>"},{"location":"architecture/overview/#at-adapter-boundaries","title":"At Adapter Boundaries","text":"<pre><code>try:\n    response = await openai_client.transcribe(...)\nexcept Exception as e:\n    raise TranscriptionError(f\"Failed to transcribe: {e}\") from e\n</code></pre>"},{"location":"architecture/overview/#in-cli-layer","title":"In CLI Layer","text":"<pre><code>try:\n    settings = Settings.load_from_file()\nexcept Exception:\n    console.print(\"[red]No API key found. Run 'shh setup' first.[/red]\")\n    raise typer.Exit(code=1)\n</code></pre>"},{"location":"architecture/overview/#resource-cleanup","title":"Resource Cleanup","text":"<pre><code>try:\n    result = await transcribe_audio(wav_path, api_key)\nfinally:\n    wav_path.unlink(missing_ok=True)  # Always clean up\n</code></pre>"},{"location":"architecture/overview/#testing-strategy","title":"Testing Strategy","text":"<p>See Testing Architecture for detailed testing approach.</p> <p>Summary:</p> <ul> <li>Unit tests: Core logic, isolated functions</li> <li>Integration tests: Adapters with mocked APIs</li> <li>No E2E tests: Avoid real API calls in CI</li> <li>80%+ coverage: Enforced via Codecov</li> </ul>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Design Decisions - Why we made specific choices</li> <li>Testing Architecture - Testing strategy and patterns</li> <li>API Reference - Detailed code documentation</li> </ul>"},{"location":"architecture/testing/","title":"Testing Architecture","text":"<p>shh uses pytest with a comprehensive testing strategy that balances coverage, speed, and reliability.</p>"},{"location":"architecture/testing/#testing-philosophy","title":"Testing Philosophy","text":"<p>Goal: 80%+ coverage with fast, reliable tests</p> <ul> <li>Unit tests: Test individual functions and classes in isolation</li> <li>Integration tests: Test full workflows with mocked external APIs</li> <li>No E2E tests: Avoid real API calls in CI (cost, speed, reliability)</li> </ul>"},{"location":"architecture/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared fixtures\n\u251c\u2500\u2500 unit/                    # Unit tests (fast, isolated)\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u2514\u2500\u2500 test_settings.py\n\u2502   \u2514\u2500\u2500 cli/\n\u2502       \u2514\u2500\u2500 test_commands.py\n\u2514\u2500\u2500 integration/             # Integration tests (mocked APIs)\n    \u2514\u2500\u2500 test_recording_flow.py\n</code></pre>"},{"location":"architecture/testing/#shared-fixtures-conftestpy","title":"Shared Fixtures (<code>conftest.py</code>)","text":""},{"location":"architecture/testing/#configuration-fixtures","title":"Configuration Fixtures","text":"<p><code>temp_config_dir</code> - Isolated config directory for tests</p> <pre><code>@pytest.fixture\ndef temp_config_dir(tmp_path: Path) -&gt; Path:\n    config_dir = tmp_path / \"config\"\n    config_dir.mkdir()\n    return config_dir\n</code></pre> <p><code>mock_settings</code> - Pre-configured Settings instance</p> <pre><code>@pytest.fixture\ndef mock_settings(temp_config_dir: Path, monkeypatch: pytest.MonkeyPatch) -&gt; Settings:\n    monkeypatch.setattr(\n        \"shh.config.settings.Settings.get_config_path\",\n        lambda: temp_config_dir / \"settings.json\",\n    )\n    settings = Settings(openai_api_key=\"sk-test-key-1234567890\")\n    settings.save_to_file()\n    return settings\n</code></pre>"},{"location":"architecture/testing/#audio-fixtures","title":"Audio Fixtures","text":"<p><code>sample_audio_1s</code> - 1 second of test audio (440Hz sine wave)</p> <pre><code>@pytest.fixture\ndef sample_audio_1s() -&gt; np.ndarray:\n    sample_rate = 16000\n    t = np.linspace(0, 1.0, sample_rate, dtype=np.float32)\n    return np.sin(2 * np.pi * 440.0 * t).astype(np.float32)\n</code></pre> <p><code>mock_audio_recorder</code> - Mocked AudioRecorder for CLI tests</p> <pre><code>@pytest.fixture\ndef mock_audio_recorder(sample_audio_1s: np.ndarray) -&gt; MagicMock:\n    mock = MagicMock()\n    mock.__aenter__.return_value = mock\n    mock.__aexit__.return_value = None\n    mock.get_audio_data.return_value = sample_audio_1s\n    return mock\n</code></pre>"},{"location":"architecture/testing/#api-mock-fixtures","title":"API Mock Fixtures","text":"<p><code>mock_whisper_response</code> - Mock OpenAI Whisper API response</p> <pre><code>@pytest.fixture\ndef mock_whisper_response() -&gt; MagicMock:\n    mock = MagicMock()\n    mock.text = \"This is a test transcription.\"\n    return mock\n</code></pre> <p><code>mock_pydantic_ai_response</code> - Mock PydanticAI formatting response</p> <pre><code>@pytest.fixture\ndef mock_pydantic_ai_response() -&gt; MagicMock:\n    mock_result = MagicMock()\n    mock_result.output.text = \"This is a formatted test.\"\n    return mock_result\n</code></pre>"},{"location":"architecture/testing/#unit-tests","title":"Unit Tests","text":""},{"location":"architecture/testing/#configuration-tests-testsunitconfigtest_settingspy","title":"Configuration Tests (<code>tests/unit/config/test_settings.py</code>)","text":"<p>What we test:</p> <ul> <li>Default values are correct</li> <li>Settings can be saved and loaded from JSON</li> <li>Enum validation (TranscriptionStyle, WhisperModel)</li> <li>Platform-specific config path</li> </ul> <p>Example:</p> <pre><code>def test_settings_defaults() -&gt; None:\n    settings = Settings()\n    assert settings.default_style == TranscriptionStyle.NEUTRAL\n    assert settings.show_progress is True\n    assert settings.whisper_model == WhisperModel.WHISPER_1\n\ndef test_save_and_load(temp_config_dir: Path, monkeypatch: pytest.MonkeyPatch) -&gt; None:\n    # Arrange\n    config_file = temp_config_dir / \"settings.json\"\n    monkeypatch.setattr(\n        \"shh.config.settings.Settings.get_config_path\",\n        lambda: config_file,\n    )\n\n    # Act\n    settings = Settings(openai_api_key=\"sk-test-key\")\n    settings.save_to_file()\n    loaded = Settings.load_from_file()\n\n    # Assert\n    assert loaded is not None\n    assert loaded.openai_api_key == \"sk-test-key\"\n</code></pre>"},{"location":"architecture/testing/#cli-command-tests-testsunitclitest_commandspy","title":"CLI Command Tests (<code>tests/unit/cli/test_commands.py</code>)","text":"<p>What we test:</p> <ul> <li>Help messages display correctly</li> <li>Setup command saves API key</li> <li>Config commands (show, get, set, reset) work correctly</li> <li>Validation errors show helpful messages</li> <li>Exit codes are correct (0 for success, 1 for errors)</li> </ul> <p>Example:</p> <pre><code>def test_setup_command(temp_config_dir: Path, monkeypatch: pytest.MonkeyPatch) -&gt; None:\n    config_file = temp_config_dir / \"settings.json\"\n    monkeypatch.setattr(\n        \"shh.config.settings.Settings.get_config_path\",\n        lambda: config_file,\n    )\n\n    # Simulate user input\n    result = runner.invoke(app, [\"setup\"], input=\"sk-test-key-12345678\\n\")\n\n    assert result.exit_code == 0\n    assert \"Setup Complete\" in result.stdout\n    assert \"sk-***5678\" in result.stdout  # Masked display\n\n    # Verify file was saved\n    settings = Settings.load_from_file()\n    assert settings is not None\n    assert settings.openai_api_key == \"sk-test-key-12345678\"\n</code></pre> <p>Key pattern: Typer's CliRunner</p> <pre><code>from typer.testing import CliRunner\n\nrunner = CliRunner()\nresult = runner.invoke(app, [\"config\", \"set\", \"default_style\", \"casual\"])\n\nassert result.exit_code == 0\nassert \"Updated default_style = casual\" in result.stdout\n</code></pre>"},{"location":"architecture/testing/#integration-tests","title":"Integration Tests","text":""},{"location":"architecture/testing/#recording-flow-tests-testsintegrationtest_recording_flowpy","title":"Recording Flow Tests (<code>tests/integration/test_recording_flow.py</code>)","text":"<p>What we test:</p> <ul> <li>Full transcription pipeline with mocked APIs</li> <li>Whisper API integration (mocked)</li> <li>PydanticAI formatting (mocked)</li> <li>Translation workflow</li> <li>Error handling (API failures)</li> </ul> <p>Example: Transcribe with Mocked Whisper API</p> <pre><code>@pytest.mark.asyncio\nasync def test_transcribe_audio_success(tmp_path: Path) -&gt; None:\n    # Create sample audio\n    audio_data = np.random.randn(16000).astype(np.float32) * 0.5\n    wav_path = save_audio_to_wav(audio_data)\n\n    try:\n        # Mock OpenAI API\n        with patch(\"shh.adapters.whisper.client.AsyncOpenAI\") as mock_client:\n            mock_transcription = MagicMock()\n            mock_transcription.text = \"Hello, this is a test.\"\n\n            mock_instance = mock_client.return_value\n            mock_instance.audio.transcriptions.create = AsyncMock(\n                return_value=mock_transcription\n            )\n\n            # Call transcribe_audio\n            result = await transcribe_audio(wav_path, \"sk-test-key\")\n\n            assert result == \"Hello, this is a test.\"\n            mock_instance.audio.transcriptions.create.assert_called_once()\n\n    finally:\n        wav_path.unlink(missing_ok=True)\n</code></pre> <p>Example: Full Pipeline Test</p> <pre><code>@pytest.mark.asyncio\nasync def test_full_pipeline_mock(tmp_path: Path) -&gt; None:\n    audio_data = np.random.randn(16000).astype(np.float32) * 0.5\n    wav_path = save_audio_to_wav(audio_data)\n\n    try:\n        # Step 1: Mock Whisper API\n        with patch(\"shh.adapters.whisper.client.AsyncOpenAI\") as mock_whisper:\n            mock_transcription = MagicMock()\n            mock_transcription.text = \"Um, this is a test transcription.\"\n\n            mock_whisper_instance = mock_whisper.return_value\n            mock_whisper_instance.audio.transcriptions.create = AsyncMock(\n                return_value=mock_transcription\n            )\n\n            # Transcribe\n            raw_text = await transcribe_audio(wav_path, \"sk-test-key\")\n            assert raw_text == \"Um, this is a test transcription.\"\n\n            # Step 2: Mock PydanticAI for formatting\n            with patch(\"shh.adapters.llm.formatter.OpenAIChatModel\"):\n                with patch(\"shh.adapters.llm.formatter.Agent\") as mock_agent_class:\n                    mock_agent = MagicMock()\n                    mock_result = MagicMock()\n                    mock_result.output.text = \"This is a test transcription.\"\n\n                    mock_agent.run = AsyncMock(return_value=mock_result)\n                    mock_agent_class.return_value = mock_agent\n\n                    # Format\n                    formatted = await format_transcription(\n                        raw_text,\n                        style=TranscriptionStyle.CASUAL,\n                        api_key=\"sk-test-key\",\n                    )\n\n                    assert formatted.text == \"This is a test transcription.\"\n\n    finally:\n        wav_path.unlink(missing_ok=True)\n</code></pre>"},{"location":"architecture/testing/#testing-async-code","title":"Testing Async Code","text":""},{"location":"architecture/testing/#pytest-asyncio","title":"pytest-asyncio","text":"<p>All async tests use <code>@pytest.mark.asyncio</code> decorator:</p> <pre><code>@pytest.mark.asyncio\nasync def test_async_function() -&gt; None:\n    result = await some_async_function()\n    assert result == expected\n</code></pre> <p>Configuration (pyproject.toml):</p> <pre><code>[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\n</code></pre>"},{"location":"architecture/testing/#mocking-async-functions","title":"Mocking Async Functions","text":"<p>Use <code>AsyncMock</code> for async functions:</p> <pre><code>from unittest.mock import AsyncMock\n\nmock_instance.audio.transcriptions.create = AsyncMock(\n    return_value=mock_transcription\n)\n</code></pre>"},{"location":"architecture/testing/#mocking-strategies","title":"Mocking Strategies","text":""},{"location":"architecture/testing/#why-we-mock-external-apis","title":"Why We Mock External APIs","text":"<ul> <li>Speed: API calls are slow (hundreds of ms), mocks are instant</li> <li>Cost: Real API calls cost money</li> <li>Reliability: External APIs can fail or rate-limit</li> <li>Isolation: Test our code, not OpenAI's</li> </ul>"},{"location":"architecture/testing/#how-we-mock","title":"How We Mock","text":"<p>1. Patch at Import Point</p> <pre><code>with patch(\"shh.adapters.whisper.client.AsyncOpenAI\") as mock_client:\n    # Setup mock\n    mock_instance = mock_client.return_value\n    mock_instance.audio.transcriptions.create = AsyncMock(...)\n\n    # Call code under test\n    result = await transcribe_audio(...)\n</code></pre> <p>2. Mock Return Values</p> <pre><code>mock_transcription = MagicMock()\nmock_transcription.text = \"Expected transcription\"\n\nmock_instance.audio.transcriptions.create = AsyncMock(\n    return_value=mock_transcription\n)\n</code></pre> <p>3. Verify Calls</p> <pre><code>result = await transcribe_audio(wav_path, \"sk-test-key\")\n\n# Verify API was called correctly\nmock_instance.audio.transcriptions.create.assert_called_once()\n\n# Verify call arguments (if needed)\ncall_args = mock_instance.audio.transcriptions.create.call_args\nassert call_args.kwargs[\"file\"] is not None\n</code></pre>"},{"location":"architecture/testing/#coverage","title":"Coverage","text":""},{"location":"architecture/testing/#target-80-coverage","title":"Target: 80%+ Coverage","text":"<p>Run coverage with:</p> <pre><code>uv run poe test-cov\n</code></pre> <p>Output:</p> <pre><code>---------- coverage: platform darwin, python 3.11.9 -----------\nName                                    Stmts   Miss  Cover\n-----------------------------------------------------------\nshh/__init__.py                             0      0   100%\nshh/adapters/audio/processor.py            12      0   100%\nshh/adapters/audio/recorder.py             45      3    93%\nshh/adapters/whisper/client.py             20      1    95%\nshh/cli/commands/config.py                 75      5    93%\nshh/config/settings.py                     35      2    94%\n-----------------------------------------------------------\nTOTAL                                     387     25    94%\n</code></pre>"},{"location":"architecture/testing/#what-we-dont-cover","title":"What We Don't Cover","text":"<ul> <li>Error paths that are hard to trigger (rare edge cases)</li> <li>CLI display code (Rich output formatting)</li> <li>Platform-specific code paths</li> </ul>"},{"location":"architecture/testing/#codecov-integration","title":"Codecov Integration","text":"<p>Coverage is uploaded to Codecov on every CI run:</p> <pre><code>- name: Upload coverage to Codecov\n  uses: codecov/codecov-action@v4\n  with:\n    token: ${{ secrets.CODECOV_TOKEN }}\n    files: ./coverage.xml\n</code></pre>"},{"location":"architecture/testing/#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"architecture/testing/#1-arrange-act-assert-pattern","title":"1. Arrange-Act-Assert Pattern","text":"<pre><code>def test_example():\n    # Arrange\n    settings = Settings(openai_api_key=\"sk-test\")\n\n    # Act\n    result = settings.openai_api_key\n\n    # Assert\n    assert result == \"sk-test\"\n</code></pre>"},{"location":"architecture/testing/#2-descriptive-test-names","title":"2. Descriptive Test Names","text":"<pre><code># \u2705 Good - describes what is tested\ndef test_setup_command_saves_api_key()\n\n# \u274c Bad - vague\ndef test_setup()\n</code></pre>"},{"location":"architecture/testing/#3-one-assertion-per-concept","title":"3. One Assertion Per Concept","text":"<pre><code># \u2705 Good\ndef test_config_set_valid():\n    result = runner.invoke(app, [\"config\", \"set\", \"default_style\", \"casual\"])\n    assert result.exit_code == 0\n    assert \"Updated default_style = casual\" in result.stdout\n\n# \u26a0\ufe0f Acceptable (related assertions)\ndef test_multiple_related_assertions():\n    settings = Settings()\n    assert settings.default_style == TranscriptionStyle.NEUTRAL\n    assert settings.show_progress is True\n</code></pre>"},{"location":"architecture/testing/#4-isolate-tests-with-fixtures","title":"4. Isolate Tests with Fixtures","text":"<pre><code>@pytest.fixture\ndef temp_config_dir(tmp_path: Path) -&gt; Path:\n    config_dir = tmp_path / \"config\"\n    config_dir.mkdir()\n    return config_dir\n\ndef test_with_isolated_config(temp_config_dir: Path):\n    # Each test gets its own config directory\n    # No interference between tests\n    pass\n</code></pre>"},{"location":"architecture/testing/#5-clean-up-resources","title":"5. Clean Up Resources","text":"<pre><code>def test_with_cleanup(tmp_path: Path):\n    wav_path = tmp_path / \"audio.wav\"\n\n    try:\n        # Use resource\n        save_audio_to_wav(audio_data, wav_path)\n    finally:\n        # Always clean up\n        wav_path.unlink(missing_ok=True)\n</code></pre>"},{"location":"architecture/testing/#running-tests","title":"Running Tests","text":""},{"location":"architecture/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code>uv run poe test\n</code></pre>"},{"location":"architecture/testing/#run-unit-tests-only","title":"Run Unit Tests Only","text":"<pre><code>uv run poe test-unit\n</code></pre>"},{"location":"architecture/testing/#run-integration-tests-only","title":"Run Integration Tests Only","text":"<pre><code>uv run poe test-integration\n</code></pre>"},{"location":"architecture/testing/#run-with-coverage","title":"Run with Coverage","text":"<pre><code>uv run poe test-cov\n</code></pre>"},{"location":"architecture/testing/#run-specific-test-file","title":"Run Specific Test File","text":"<pre><code>pytest tests/unit/config/test_settings.py\n</code></pre>"},{"location":"architecture/testing/#run-specific-test","title":"Run Specific Test","text":"<pre><code>pytest tests/unit/config/test_settings.py::test_settings_defaults\n</code></pre>"},{"location":"architecture/testing/#verbose-output","title":"Verbose Output","text":"<pre><code>pytest -v\n</code></pre>"},{"location":"architecture/testing/#ci-integration","title":"CI Integration","text":"<p>Tests run on every push and PR via GitHub Actions:</p> <pre><code>- name: Run type checking\n  run: uv run poe type\n\n- name: Run linting\n  run: uv run poe lint\n\n- name: Run tests with coverage\n  run: uv run poe test-cov\n</code></pre> <p>Matrix testing:</p> <ul> <li>OS: Ubuntu, macOS, Windows</li> <li>Python: 3.11, 3.12, 3.13</li> </ul> <p>This ensures cross-platform compatibility.</p>"},{"location":"architecture/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Overview - High-level architecture</li> <li>Design Decisions - Why we made specific choices</li> <li>API Reference - Detailed code documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Requires Python 3.11 or higher.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11, 3.12, or 3.13</li> <li>OpenAI API key (get one here)</li> <li>Microphone (for recording)</li> </ul>"},{"location":"getting-started/installation/#using-uv","title":"Using uv","text":"<pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Install shh:</p> <pre><code># Clone the repository\ngit clone https://github.com/mpruvot/shh.git\ncd shh\n\n# Install in editable mode\nuv pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code># Clone the repository\ngit clone https://github.com/mpruvot/shh.git\ncd shh\n\n# Create a virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development, install with dev dependencies:</p> <pre><code># Using uv\nuv pip install -e \".[dev]\"\n\n# Using pip\npip install -e \".[dev]\"\n</code></pre> <p>This includes: - pytest and testing tools - mypy for type checking - ruff for linting and formatting - pre-commit hooks</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>shh --help\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configure your API key</li> <li>Record your first transcription</li> </ul>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#command-not-found","title":"Command not found","text":"<p>If <code>shh</code> isn't found, ensure your virtual environment is activated:</p> <pre><code>source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/installation/#audio-issues","title":"Audio issues","text":"<p>If recording doesn't work, check your microphone permissions:</p> <ul> <li>macOS: System Preferences \u2192 Security &amp; Privacy \u2192 Microphone</li> <li>Linux: Ensure ALSA or PulseAudio is configured</li> <li>Windows: Settings \u2192 Privacy \u2192 Microphone</li> </ul>"},{"location":"getting-started/installation/#import-errors","title":"Import errors","text":"<p>If you see import errors, reinstall dependencies:</p> <pre><code>uv pip install -e .  # or pip install -e .\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#setup","title":"Setup","text":"<p>After installation, configure your API key:</p> <pre><code>shh setup\n</code></pre> <p>API Key Storage</p> <p>Your API key is stored locally in:</p> <ul> <li>macOS: <code>~/Library/Application Support/shh/config.json</code></li> <li>Linux: <code>~/.config/shh/config.json</code></li> <li>Windows: <code>%APPDATA%\\shh\\config.json</code></li> </ul>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#record-and-transcribe","title":"Record and Transcribe","text":"<pre><code>shh\n</code></pre> <p>Press Enter to stop. Output appears in terminal and clipboard.</p>"},{"location":"getting-started/quickstart/#record-with-duration","title":"Record with Duration","text":"<pre><code>shh --duration 60\n</code></pre>"},{"location":"getting-started/quickstart/#formatting-styles","title":"Formatting Styles","text":"<pre><code># Casual style - conversational, filler words removed\nshh --style casual\n\n# Business style - professional, formal tone\nshh --style business\n\n# Neutral style - no formatting, raw Whisper output\nshh --style neutral\n</code></pre> <p>Style Comparison</p> <p>Raw Whisper output:</p> <p>Um, so like, I was thinking we should probably update the database schema, you know?</p> <p>Casual style:</p> <p>I was thinking we should update the database schema.</p> <p>Business style:</p> <p>I recommend updating the database schema to improve data integrity and performance.</p>"},{"location":"getting-started/quickstart/#translation","title":"Translation","text":"<pre><code># Transcribe French audio to English\nshh --translate English\n\n# Combine with formatting\nshh --style business --translate English\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<pre><code># View settings\nshh config show\n\n# Set default style\nshh config set default_style casual\n</code></pre>"},{"location":"getting-started/quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quickstart/#quick-voice-note","title":"Quick Voice Note","text":"<pre><code>shh\n# Press Enter when done\n# Result is in clipboard - paste anywhere\n</code></pre>"},{"location":"getting-started/quickstart/#meeting-transcription","title":"Meeting Transcription","text":"<pre><code># Business formatting for professional output\nshh --style business\n</code></pre>"},{"location":"getting-started/quickstart/#multilingual-interview","title":"Multilingual Interview","text":"<pre><code># Record French, transcribe to English, business format\nshh --style business --translate English\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn all commands</li> <li>Customize configuration</li> <li>Understand formatting styles</li> </ul>"},{"location":"getting-started/quickstart/#tips","title":"Tips","text":"<p>Clipboard Integration</p> <p>Results are automatically copied to clipboard.</p> <p>Progress Indicator</p> <p>Live timer shows elapsed time while recording.</p> <p>Maximum Duration</p> <p>Default limit: 5 minutes. Use multiple sessions for longer recordings.</p>"},{"location":"user-guide/commands/","title":"Commands Reference","text":"<p>Complete reference for all shh commands.</p>"},{"location":"user-guide/commands/#default-command","title":"Default Command","text":""},{"location":"user-guide/commands/#shh","title":"<code>shh</code>","text":"<p>Start recording immediately. Press Enter to stop.</p> <pre><code>shh\n</code></pre> <p>Options:</p> <ul> <li><code>--style, -s</code> - Formatting style (neutral, casual, business)</li> <li><code>--translate, -t</code> - Target language for translation</li> <li><code>--duration, -d</code> - Recording duration in seconds (optional)</li> </ul> <p>Examples:</p> <pre><code># Basic recording\nshh\n\n# With casual formatting\nshh --style casual\n\n# Record for 60 seconds\nshh --duration 60\n\n# Translate to French with business formatting\nshh --style business --translate French\n</code></pre>"},{"location":"user-guide/commands/#setup-command","title":"Setup Command","text":""},{"location":"user-guide/commands/#shh-setup","title":"<code>shh setup</code>","text":"<p>Interactive setup wizard to configure your OpenAI API key.</p> <pre><code>shh setup\n</code></pre> <p>Your API key is stored securely in a platform-specific location. The key is hidden while typing for security.</p> <p>What it does:</p> <ol> <li>Prompts for your OpenAI API key (input hidden)</li> <li>Validates the key is not empty</li> <li>Saves to config file</li> <li>Displays masked key confirmation (e.g., <code>sk-***xyz</code>)</li> </ol> <p>Example:</p> <pre><code>$ shh setup\nEnter your OpenAI API key: ****************\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503           Setup Complete                    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\nConfiguration saved successfully!\nConfig file: /Users/you/Library/Application Support/shh/config.json\nSettings:\n  \u2022 OpenAI API Key: sk-***xyz\n  \u2022 Default style: neutral\n  \u2022 Show progress: true\n</code></pre>"},{"location":"user-guide/commands/#config-commands","title":"Config Commands","text":""},{"location":"user-guide/commands/#shh-config-show","title":"<code>shh config show</code>","text":"<p>Display all current configuration settings.</p> <pre><code>shh config show\n</code></pre> <p>Example output:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Setting         \u2503 Value                    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 openai_api_key  \u2502 sk-***xyz                \u2502\n\u2502 default_style   \u2502 casual                   \u2502\n\u2502 show_progress   \u2502 True                     \u2502\n\u2502 whisper_model   \u2502 whisper-1                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/commands/#shh-config-get-key","title":"<code>shh config get &lt;key&gt;</code>","text":"<p>Get a specific configuration value.</p> <pre><code>shh config get default_style\n</code></pre> <p>Available keys:</p> <ul> <li><code>openai_api_key</code></li> <li><code>default_style</code></li> <li><code>show_progress</code></li> <li><code>whisper_model</code></li> </ul> <p>Example:</p> <pre><code>$ shh config get default_style\ndefault_style: casual\n</code></pre>"},{"location":"user-guide/commands/#shh-config-set-key-value","title":"<code>shh config set &lt;key&gt; &lt;value&gt;</code>","text":"<p>Update a configuration setting.</p> <pre><code>shh config set default_style casual\n</code></pre> <p>Examples:</p> <pre><code># Set default formatting style\nshh config set default_style business\n\n# Enable/disable progress indicator\nshh config set show_progress true\n\n# Change Whisper model (if needed)\nshh config set whisper_model whisper-1\n</code></pre> <p>Validation:</p> <p>The command validates input and provides helpful error messages:</p> <pre><code>$ shh config set default_style invalid\nError: Invalid style 'invalid'\nValid styles: neutral, casual, business\n</code></pre>"},{"location":"user-guide/commands/#shh-config-reset","title":"<code>shh config reset</code>","text":"<p>Reset all settings to defaults (requires confirmation).</p> <pre><code>shh config reset\n</code></pre> <p>You'll be prompted to confirm:</p> <pre><code>$ shh config reset\nThis will reset all configuration to defaults.\nContinue? [y/N]: y\n\nConfiguration reset to defaults.\nRun 'shh setup' to configure your API key.\n</code></pre> <p>What gets reset:</p> <ul> <li><code>default_style</code> \u2192 neutral</li> <li><code>show_progress</code> \u2192 true</li> <li><code>whisper_model</code> \u2192 whisper-1</li> <li>Note: Your API key is also cleared - you'll need to run <code>shh setup</code> again</li> </ul>"},{"location":"user-guide/commands/#global-options","title":"Global Options","text":"<p>All commands support these options:</p> <ul> <li><code>--help</code> - Show help message</li> <li><code>--version</code> - Show version information</li> </ul> <p>Examples:</p> <pre><code># Get help for any command\nshh --help\nshh config --help\nshh config set --help\n\n# Check version\nshh --version\n</code></pre>"},{"location":"user-guide/commands/#exit-codes","title":"Exit Codes","text":"<p>shh uses standard exit codes:</p> <ul> <li><code>0</code> - Success</li> <li><code>1</code> - Error (invalid input, API failure, etc.)</li> </ul> <p>Use these in scripts:</p> <pre><code>if shh --style casual; then\n    echo \"Transcription successful\"\nelse\n    echo \"Transcription failed\"\nfi\n</code></pre>"},{"location":"user-guide/commands/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration details</li> <li>Understanding formatting styles</li> <li>Translation guide</li> </ul>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>shh stores configuration in a JSON file at a platform-specific location. You can also use environment variables or command-line flags.</p>"},{"location":"user-guide/configuration/#configuration-file-location","title":"Configuration File Location","text":"<pre><code>macOS:    ~/Library/Application Support/shh/config.json\nLinux:    ~/.config/shh/config.json\nWindows:  %APPDATA%\\shh\\config.json\n</code></pre> <p>The file is created automatically when you run <code>shh setup</code>.</p>"},{"location":"user-guide/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Settings are loaded in this order (highest priority first):</p> <ol> <li>Command-line flags - <code>--style casual</code></li> <li>Environment variables - <code>SHH_DEFAULT_STYLE=casual</code></li> <li>Config file - <code>config.json</code></li> <li>Defaults - Built-in defaults</li> </ol>"},{"location":"user-guide/configuration/#available-settings","title":"Available Settings","text":""},{"location":"user-guide/configuration/#openai_api_key","title":"<code>openai_api_key</code>","text":"<p>Type: String Required: Yes Default: None</p> <p>Your OpenAI API key for Whisper and GPT APIs.</p> <pre><code># Set via setup command (recommended)\nshh setup\n\n# Set via environment variable\nexport SHH_OPENAI_API_KEY=\"sk-...\"\n\n# Set via config command\nshh config set openai_api_key \"sk-...\"\n</code></pre> <p>Security</p> <p>Never commit your API key to version control. Use environment variables or the config file.</p>"},{"location":"user-guide/configuration/#default_style","title":"<code>default_style</code>","text":"<p>Type: String Options: <code>neutral</code>, <code>casual</code>, <code>business</code> Default: <code>neutral</code></p> <p>Default formatting style for transcriptions.</p> <pre><code># Set via config command\nshh config set default_style casual\n\n# Set via environment variable\nexport SHH_DEFAULT_STYLE=casual\n\n# Override via flag\nshh --style business\n</code></pre> <p>See Formatting Styles for details on each style.</p>"},{"location":"user-guide/configuration/#default_translation_language","title":"<code>default_translation_language</code>","text":"<p>Type: String Options: Any language name (e.g., <code>English</code>, <code>French</code>, <code>Spanish</code>) Default: None</p> <p>Default language to translate transcriptions to. When set, all recordings are automatically translated to this language unless overridden with <code>--translate</code>.</p> <pre><code># Set default translation language\nshh config set default_translation_language English\n\n# Now all recordings auto-translate\nshh\n\n# Override when needed\nshh --translate French\n\n# Clear default translation\nshh config set default_translation_language none\n</code></pre> <p>See Translation for details on supported languages.</p>"},{"location":"user-guide/configuration/#show_progress","title":"<code>show_progress</code>","text":"<p>Type: Boolean Options: <code>true</code>, <code>false</code> Default: <code>true</code></p> <p>Show live progress indicator while recording.</p> <pre><code># Set via config command\nshh config set show_progress false\n\n# Set via environment variable\nexport SHH_SHOW_PROGRESS=false\n</code></pre> <p>When enabled, you'll see: <pre><code>\ud83d\udd34 Recording... 12.3s [Press Enter to stop]\n</code></pre></p> <p>When disabled, recording is silent until you press Enter.</p>"},{"location":"user-guide/configuration/#whisper_model","title":"<code>whisper_model</code>","text":"<p>Type: String Options: <code>whisper-1</code> Default: <code>whisper-1</code></p> <p>OpenAI Whisper model to use for transcription.</p> <pre><code># Set via config command\nshh config set whisper_model whisper-1\n\n# Set via environment variable\nexport SHH_WHISPER_MODEL=whisper-1\n</code></pre> <p>Model Availability</p> <p>Currently, <code>whisper-1</code> is the only available model via the OpenAI API. This setting exists for future compatibility.</p>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>All settings can be configured via environment variables with the <code>SHH_</code> prefix:</p> <pre><code>export SHH_OPENAI_API_KEY=\"sk-...\"\nexport SHH_DEFAULT_STYLE=\"casual\"\nexport SHH_DEFAULT_TRANSLATION_LANGUAGE=\"English\"\nexport SHH_SHOW_PROGRESS=\"true\"\nexport SHH_WHISPER_MODEL=\"whisper-1\"\n</code></pre> <p>Add these to your shell profile (<code>~/.bashrc</code>, <code>~/.zshrc</code>, etc.) for persistence.</p>"},{"location":"user-guide/configuration/#example-configuration-file","title":"Example Configuration File","text":"<p>Here's what a typical <code>config.json</code> looks like:</p> <pre><code>{\n  \"openai_api_key\": \"sk-proj-...\",\n  \"default_style\": \"casual\",\n  \"default_translation_language\": \"English\",\n  \"show_progress\": true,\n  \"whisper_model\": \"whisper-1\"\n}\n</code></pre>"},{"location":"user-guide/configuration/#managing-configuration","title":"Managing Configuration","text":""},{"location":"user-guide/configuration/#view-current-config","title":"View Current Config","text":"<pre><code>shh config show\n</code></pre>"},{"location":"user-guide/configuration/#get-single-setting","title":"Get Single Setting","text":"<pre><code>shh config get default_style\n</code></pre>"},{"location":"user-guide/configuration/#update-setting","title":"Update Setting","text":"<pre><code>shh config set default_style business\n</code></pre>"},{"location":"user-guide/configuration/#reset-to-defaults","title":"Reset to Defaults","text":"<pre><code>shh config reset\n</code></pre> <p>Reset Warning</p> <p><code>shh config reset</code> clears all settings, including your API key. You'll need to run <code>shh setup</code> again.</p>"},{"location":"user-guide/configuration/#common-configurations","title":"Common Configurations","text":""},{"location":"user-guide/configuration/#casual-user","title":"Casual User","text":"<p>Always use casual formatting, hide progress:</p> <pre><code>shh config set default_style casual\nshh config set show_progress false\n</code></pre> <p>Or in environment variables:</p> <pre><code>export SHH_DEFAULT_STYLE=casual\nexport SHH_SHOW_PROGRESS=false\n</code></pre>"},{"location":"user-guide/configuration/#business-user","title":"Business User","text":"<p>Professional formatting by default:</p> <pre><code>shh config set default_style business\n</code></pre>"},{"location":"user-guide/configuration/#developer-testing","title":"Developer Testing","text":"<p>Use environment variables to avoid modifying config:</p> <pre><code>SHH_DEFAULT_STYLE=casual shh\n</code></pre>"},{"location":"user-guide/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/#config-file-not-found","title":"Config file not found","text":"<p>If you see \"No API key found\", run:</p> <pre><code>shh setup\n</code></pre> <p>This creates the config file and prompts for your API key.</p>"},{"location":"user-guide/configuration/#permission-errors","title":"Permission errors","text":"<p>If you can't write to the config file, check directory permissions:</p> <pre><code># macOS/Linux\nls -la ~/Library/Application\\ Support/shh/  # macOS\nls -la ~/.config/shh/  # Linux\n\n# Windows\ndir %APPDATA%\\shh\\\n</code></pre>"},{"location":"user-guide/configuration/#invalid-values","title":"Invalid values","text":"<p>If you manually edit <code>config.json</code> and enter invalid values, shh will show an error:</p> <pre><code>Error: Invalid style 'foo'\nValid styles: neutral, casual, business\n</code></pre> <p>Fix by running:</p> <pre><code>shh config set default_style neutral\n</code></pre>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about formatting styles</li> <li>Translation options</li> <li>All commands reference</li> </ul>"},{"location":"user-guide/styles/","title":"Formatting Styles","text":"<p>shh can format your transcriptions using AI to match different contexts. Choose a style based on how you plan to use the transcription.</p>"},{"location":"user-guide/styles/#available-styles","title":"Available Styles","text":""},{"location":"user-guide/styles/#neutral","title":"Neutral","text":"<p>Best for: Accuracy, raw data, archival</p> <p>Neutral style returns the transcription exactly as Whisper outputs it, with no AI formatting applied. This is the fastest option and preserves the original phrasing.</p> <p>When to use:</p> <ul> <li>You need verbatim transcriptions</li> <li>You'll edit the text yourself</li> <li>You want to minimize API costs (no GPT call)</li> <li>Accuracy matters more than readability</li> </ul> <p>Example:</p> <pre><code>shh --style neutral\n</code></pre> <p>Input (spoken):</p> <p>Um, so like, I think we should probably update the database schema, you know, to improve performance and stuff.</p> <p>Output:</p> <p>Um, so like, I think we should probably update the database schema, you know, to improve performance and stuff.</p>"},{"location":"user-guide/styles/#casual","title":"Casual","text":"<p>Best for: Quick notes, personal use, conversational context</p> <p>Casual style removes filler words and smooths out the transcription while maintaining a conversational tone. It's like how you'd text a colleague.</p> <p>When to use:</p> <ul> <li>Taking quick notes or voice memos</li> <li>Internal team communication</li> <li>Personal documentation</li> <li>You want readability without formality</li> </ul> <p>Example:</p> <pre><code>shh --style casual\n</code></pre> <p>Input (spoken):</p> <p>Um, so like, I think we should probably update the database schema, you know, to improve performance and stuff.</p> <p>Output:</p> <p>I think we should update the database schema to improve performance.</p> <p>What it does:</p> <ul> <li>Removes filler words (um, uh, like, you know)</li> <li>Fixes minor grammatical issues</li> <li>Maintains conversational tone</li> <li>Keeps it concise but friendly</li> </ul>"},{"location":"user-guide/styles/#business","title":"Business","text":"<p>Best for: Professional documentation, formal communication, presentations</p> <p>Business style transforms your transcription into polished, professional text suitable for formal contexts. It adds structure and clarity.</p> <p>When to use:</p> <ul> <li>Meeting minutes or reports</li> <li>Documentation for stakeholders</li> <li>Professional emails or messages</li> <li>Presentations or proposals</li> </ul> <p>Example:</p> <pre><code>shh --style business\n</code></pre> <p>Input (spoken):</p> <p>Um, so like, I think we should probably update the database schema, you know, to improve performance and stuff.</p> <p>Output:</p> <p>I recommend updating the database schema to improve performance and data integrity. This will provide measurable benefits to system efficiency.</p> <p>What it does:</p> <ul> <li>Removes all filler words</li> <li>Restructures sentences for clarity</li> <li>Uses professional vocabulary</li> <li>Adds context and structure</li> <li>May expand on ideas for completeness</li> </ul>"},{"location":"user-guide/styles/#setting-a-default-style","title":"Setting a Default Style","text":"<p>If you always use the same style, set it as your default:</p> <pre><code># Set casual as default\nshh config set default_style casual\n\n# Now 'shh' automatically uses casual formatting\nshh\n</code></pre> <p>Override the default anytime with the <code>--style</code> flag:</p> <pre><code># Even with casual as default, use business for this recording\nshh --style business\n</code></pre>"},{"location":"user-guide/styles/#style-comparison","title":"Style Comparison","text":"Feature Neutral Casual Business Filler words Kept Removed Removed Tone Verbatim Conversational Professional Structure Original Light cleanup Polished Speed Fastest Fast Slower API calls 1 (Whisper) 2 (Whisper + GPT) 2 (Whisper + GPT) Cost Lowest Medium Medium"},{"location":"user-guide/styles/#combined-with-translation","title":"Combined with Translation","text":"<p>Styles work seamlessly with translation:</p> <pre><code># Casual French transcription\nshh --style casual --translate French\n\n# Business English transcription from French audio\nshh --style business --translate English\n</code></pre> <p>The formatting is applied after translation, ensuring natural phrasing in the target language.</p>"},{"location":"user-guide/styles/#technical-details","title":"Technical Details","text":""},{"location":"user-guide/styles/#how-it-works","title":"How It Works","text":"<ol> <li>Neutral: Whisper API only \u2192 Direct output</li> <li>Casual/Business: Whisper API \u2192 PydanticAI agent \u2192 Formatted output</li> </ol>"},{"location":"user-guide/styles/#ai-model","title":"AI Model","text":"<p>Formatting uses OpenAI's <code>gpt-4o-mini</code> model for cost-effective, high-quality results. This is separate from the Whisper API call.</p>"},{"location":"user-guide/styles/#prompts","title":"Prompts","text":"<p>The formatting agent receives context about the desired style and applies transformations accordingly. Prompts are optimized for:</p> <ul> <li>Preserving meaning and intent</li> <li>Removing only true filler words (not meaningful pauses)</li> <li>Maintaining speaker's voice in casual mode</li> <li>Professional polish in business mode</li> </ul>"},{"location":"user-guide/styles/#privacy","title":"Privacy","text":"<p>Both Whisper and GPT API calls are processed by OpenAI. If privacy is a concern:</p> <ul> <li>Use <code>--style neutral</code> (Whisper only)</li> <li>Consider self-hosted alternatives</li> <li>Review OpenAI's privacy policy</li> </ul>"},{"location":"user-guide/styles/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/styles/#for-voice-memos","title":"For Voice Memos","text":"<p>Use casual style - it cleans up your thoughts without over-formalizing them.</p> <pre><code>shh config set default_style casual\n</code></pre>"},{"location":"user-guide/styles/#for-meetings","title":"For Meetings","text":"<p>Use business style - creates professional documentation ready to share.</p> <pre><code>shh --style business\n</code></pre>"},{"location":"user-guide/styles/#for-transcription-archives","title":"For Transcription Archives","text":"<p>Use neutral style - preserves exact phrasing for future reference.</p> <pre><code>shh --style neutral\n</code></pre>"},{"location":"user-guide/styles/#cost-optimization","title":"Cost Optimization","text":"<p>If you're watching API costs:</p> <ul> <li>Use <code>neutral</code> for drafts, then manually edit</li> <li>Use <code>casual/business</code> only for final versions</li> <li>Set <code>neutral</code> as default, override when needed</li> </ul>"},{"location":"user-guide/styles/#next-steps","title":"Next Steps","text":"<ul> <li>Translation guide</li> <li>Configuration options</li> <li>All commands reference</li> </ul>"},{"location":"user-guide/translation/","title":"Translation","text":"<p>shh can transcribe audio in one language and translate it to another, all in a single command.</p>"},{"location":"user-guide/translation/#basic-usage","title":"Basic Usage","text":"<p>Use the <code>--translate</code> flag with your target language:</p> <pre><code># Transcribe French audio and translate to English\nshh --translate English\n\n# Transcribe English audio and translate to Spanish\nshh --translate Spanish\n</code></pre>"},{"location":"user-guide/translation/#default-translation-language","title":"Default Translation Language","text":"<p>Set a default translation language to avoid typing <code>--translate</code> every time:</p> <pre><code># Set default translation language\nshh config set default_translation_language English\n\n# Now all recordings auto-translate to English\nshh\n\n# Override default when needed\nshh --translate French\n</code></pre> <p>This is useful when you frequently translate to the same language.</p>"},{"location":"user-guide/translation/#supported-languages","title":"Supported Languages","text":"<p>You can translate to any language supported by OpenAI's GPT models. Common examples:</p> <ul> <li>English</li> <li>Spanish (Espa\u00f1ol)</li> <li>French (Fran\u00e7ais)</li> <li>German (Deutsch)</li> <li>Italian (Italiano)</li> <li>Portuguese (Portugu\u00eas)</li> <li>Chinese (\u4e2d\u6587)</li> <li>Japanese (\u65e5\u672c\u8a9e)</li> <li>Korean (\ud55c\uad6d\uc5b4)</li> <li>Arabic (\u0627\u0644\u0639\u0631\u0628\u064a\u0629)</li> <li>Russian (\u0420\u0443\u0441\u0441\u043a\u0438\u0439)</li> <li>Hindi (\u0939\u093f\u0928\u094d\u0926\u0940)</li> </ul> <p>And many more. Just specify the language name in English.</p>"},{"location":"user-guide/translation/#how-it-works","title":"How It Works","text":"<ol> <li>Whisper API transcribes the audio (in original language)</li> <li>PydanticAI translates the transcription to target language</li> <li>Result appears in terminal and clipboard</li> </ol> <p>Language Detection</p> <p>Whisper automatically detects the source language. You don't need to specify it - just provide the target language for translation.</p>"},{"location":"user-guide/translation/#combining-with-styles","title":"Combining with Styles","text":"<p>Translation works seamlessly with formatting styles:</p> <pre><code># Casual translation\nshh --style casual --translate French\n\n# Business translation\nshh --style business --translate English\n</code></pre> <p>The order of operations:</p> <ol> <li>Transcribe (Whisper)</li> <li>Translate (if <code>--translate</code> specified)</li> <li>Format (if <code>--style</code> is not neutral)</li> </ol> <p>This ensures natural phrasing in the target language.</p>"},{"location":"user-guide/translation/#examples","title":"Examples","text":""},{"location":"user-guide/translation/#meeting-notes-french-english","title":"Meeting Notes (French \u2192 English)","text":"<p>Record a French meeting and get English business-formatted notes:</p> <pre><code>shh --style business --translate English\n</code></pre>"},{"location":"user-guide/translation/#voice-memo-english-spanish","title":"Voice Memo (English \u2192 Spanish)","text":"<p>Quick personal note translated to Spanish:</p> <pre><code>shh --style casual --translate Spanish\n</code></pre>"},{"location":"user-guide/translation/#interview-transcription-chinese-english","title":"Interview Transcription (Chinese \u2192 English)","text":"<p>Transcribe a Chinese interview to English:</p> <pre><code>shh --translate English\n</code></pre>"},{"location":"user-guide/translation/#language-names","title":"Language Names","text":"<p>You can use various forms of language names:</p> <pre><code># These all work\nshh --translate English\nshh --translate french\nshh --translate Espa\u00f1ol\nshh --translate \u4e2d\u6587\n</code></pre> <p>The AI model understands common language names in multiple forms.</p>"},{"location":"user-guide/translation/#quality-and-accuracy","title":"Quality and Accuracy","text":""},{"location":"user-guide/translation/#whisper-transcription","title":"Whisper Transcription","text":"<p>Whisper supports 90+ languages with high accuracy. Some languages perform better than others:</p> <ul> <li>Excellent: English, Spanish, French, German, Italian, Portuguese</li> <li>Good: Chinese, Japanese, Korean, Russian, Arabic, Hindi</li> <li>Varying: Less common languages (quality depends on audio)</li> </ul>"},{"location":"user-guide/translation/#translation-quality","title":"Translation Quality","text":"<p>Translation uses OpenAI's <code>gpt-4o-mini</code>, which provides:</p> <ul> <li>Contextually aware translations</li> <li>Natural phrasing in target language</li> <li>Preservation of meaning and tone</li> <li>Handling of idioms and cultural context</li> </ul>"},{"location":"user-guide/translation/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/translation/#clear-audio","title":"Clear Audio","text":"<p>Translation quality depends on transcription accuracy. Ensure:</p> <ul> <li>Minimal background noise</li> <li>Clear pronunciation</li> <li>Good microphone placement</li> </ul>"},{"location":"user-guide/translation/#specify-style","title":"Specify Style","text":"<p>Combining styles with translation produces better results:</p> <pre><code># \u2705 Good - clear intent\nshh --style business --translate English\n\n# \u26a0\ufe0f Acceptable - but less polished\nshh --translate English\n</code></pre>"},{"location":"user-guide/translation/#review-output","title":"Review Output","text":"<p>AI translation is good but not perfect. Review important translations:</p> <ul> <li>Technical terms may need correction</li> <li>Cultural context might be lost</li> <li>Idioms may not translate directly</li> </ul>"},{"location":"user-guide/translation/#common-use-cases","title":"Common Use Cases","text":""},{"location":"user-guide/translation/#multilingual-teams","title":"Multilingual Teams","text":"<p>Team members speak different languages:</p> <pre><code># French speaker recording notes for English team\nshh --style business --translate English\n</code></pre>"},{"location":"user-guide/translation/#learning-languages","title":"Learning Languages","text":"<p>Practice speaking and get translations:</p> <pre><code># Practice Spanish, get English translation to check understanding\nshh --translate English\n</code></pre>"},{"location":"user-guide/translation/#content-creation","title":"Content Creation","text":"<p>Transcribe and translate interviews, podcasts, or videos:</p> <pre><code># Transcribe Japanese podcast to English\nshh --translate English\n</code></pre>"},{"location":"user-guide/translation/#travel-notes","title":"Travel Notes","text":"<p>Quick voice notes while traveling, translated to your native language:</p> <pre><code># Record in local language, translate to English\nshh --translate English\n</code></pre>"},{"location":"user-guide/translation/#technical-details","title":"Technical Details","text":""},{"location":"user-guide/translation/#api-calls","title":"API Calls","text":"<p>Translation requires two API calls:</p> <ol> <li>Whisper API for transcription</li> <li>GPT-4o-mini API for translation</li> </ol> <p>Cost: Slightly higher than transcription alone, but still cost-effective with <code>gpt-4o-mini</code>.</p>"},{"location":"user-guide/translation/#language-detection","title":"Language Detection","text":"<p>Whisper automatically detects the source language. If you need to know what language was detected, check the Whisper API response (not currently exposed in CLI, but available in the code).</p>"},{"location":"user-guide/translation/#privacy","title":"Privacy","text":"<p>Both Whisper and GPT API calls are processed by OpenAI. Audio and transcriptions are sent to OpenAI servers. Review OpenAI's privacy policy if privacy is a concern.</p>"},{"location":"user-guide/translation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/translation/#translation-not-working","title":"Translation Not Working","text":"<p>If translation fails:</p> <ol> <li>Check your API key has GPT access</li> <li>Verify the language name is correct</li> <li>Ensure you have sufficient API credits</li> </ol>"},{"location":"user-guide/translation/#poor-translation-quality","title":"Poor Translation Quality","text":"<p>If translations are inaccurate:</p> <ul> <li>Check the source transcription for errors</li> <li>Use <code>--style</code> flag for better context</li> <li>Ensure clear audio quality</li> <li>Try rephrasing if speaking</li> </ul>"},{"location":"user-guide/translation/#wrong-language-detected","title":"Wrong Language Detected","text":"<p>If Whisper detects the wrong source language:</p> <ul> <li>Speak more clearly</li> <li>Reduce background noise</li> <li>Ensure sufficient audio duration</li> </ul>"},{"location":"user-guide/translation/#next-steps","title":"Next Steps","text":"<ul> <li>Formatting styles explained</li> <li>Configuration options</li> <li>All commands reference</li> </ul>"}]}